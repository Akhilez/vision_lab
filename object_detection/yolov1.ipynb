{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YOLO v1\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "Key points:\n",
    "\n",
    "- S x S grid. S = 7\n",
    "- predicts B boxes for each cell. B = 2\n",
    "- Responsible cell:\n",
    "    - the cell that contains bbox midpoint.\n",
    "    - Among B predicted boxes, only the one that has highest IoU will be responsible.\n",
    "- predicts confidence each cell. confidence = IoU\n",
    "- predicts x, y, w, h each cell:\n",
    "    - x, y: they are midpoint coordinates relative to cell origin, h, w.\n",
    "        Meaning, cell h, w are 1, 1, and x, y will be in [0, 1]\n",
    "    - h, w: they are bbox height, width relative to whole image.\n",
    "- predicts C classes each cell.\n",
    "- All are trained only when the cell is responsible for a bbox.\n",
    "- Each cell can only predict 1 object. although it tries to predict B bboxes\n",
    "- Predicted tensor is of shape [S, S, (C + 5B)]\n",
    "- Architecture is simply a CNN followed by a flatten and fully-connected layers.\n",
    "- While inference, multiply C probabilities with predicted confidence.\n",
    "- While inference, apply NMS\n",
    "- All losses are MSE variations.\n",
    "\n",
    "Hyperparams:\n",
    "\n",
    "- leaky relu\n",
    "- batch size 64\n",
    "- epochs 135 (with pre-trained)\n",
    "- momentum 0.9\n",
    "- decay: 0.0005\n",
    "- lr:\n",
    "    - 10^-3 for few epochs.\n",
    "    - 10^-2 for +75 epochs\n",
    "    - 10^-3 for +30 epochs.\n",
    "    - 10^-4 for +30 epochs.\n",
    "- Extensive augmentation:\n",
    "    - Random scaling and translation up to 20%\n",
    "    - randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space.\n",
    "- dropout of 0.5 on last fully-connected\n",
    "\n",
    "Losses:\n",
    "\n",
    "- Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "- Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "- Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "- No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "- Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "confidence = IoU\n",
    "lambda_coord = 5\n",
    "lambda_no_object = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCDetection\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Union, Optional, Tuple\n",
    "import pytorch_lightning as pl\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning albumentations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "class VocYoloDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int,\n",
    "        batch_size: int,\n",
    "        data_path: str,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        data_augment=False,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = 448\n",
    "        self.w = 448\n",
    "        self.dims = (3, self.h, self.w)\n",
    "        self.num_classes = 20\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = dataloader_num_workers\n",
    "        self.augment = data_augment\n",
    "\n",
    "        self.transforms = self._get_transforms()\n",
    "\n",
    "        self.dataset_train, self.dataset_val = None, None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"trainval\",\n",
    "            download=True,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.dataset_train = VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"train\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "        )\n",
    "        self.dataset_val = VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"val\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "        )\n",
    "\n",
    "    def train_loader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_loader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def _get_transforms(self):\n",
    "        \"\"\"\n",
    "        The transform function takes in pil image and a dict of target bboxes.\n",
    "        It applies augmentations and returns an image and target tensor of shape (C+5, S, S)\n",
    "        The transform will return image tensor and target tensor.\n",
    "\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        The output target will be a tensor of shape: (C+5, S, S)\n",
    "        :return: Callable function\n",
    "        \"\"\"\n",
    "        albument_transforms = self._get_augmentations(self.h, self.w, self.augment)\n",
    "\n",
    "        def transforms(image, targets: dict):\n",
    "            boxes, classes = self._transform_pre_augmentation(targets)\n",
    "\n",
    "            transformed = albument_transforms(\n",
    "                image=np.array(image),\n",
    "                bboxes=boxes,\n",
    "                class_labels=classes,\n",
    "            )\n",
    "\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            classes = transformed[\"class_labels\"]\n",
    "\n",
    "            targets = self._transform_targets(\n",
    "                boxes, classes, self.num_classes, self.grid_size\n",
    "            )\n",
    "            return image, targets\n",
    "\n",
    "        return transforms\n",
    "\n",
    "    def _transform_targets(\n",
    "        self, boxes, classes, num_classes, grid_size\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - Get responsible pairs:\n",
    "            - Find midpoints of all bboxes.\n",
    "            - For all cells, if there's a bbox midpoint in the cell,\n",
    "              that cell and bbox will go in a responsible pair list.\n",
    "        - Convert coordinates from (xmin, ymin, ...) to yolo style.\n",
    "        - Put everything in a tensor.\n",
    "\n",
    "        :param boxes: list of tuples of (xmin, ymin, xmax, ymax)\n",
    "        :param classes: list of integers\n",
    "        :return: torch.Tensor of shape (C+5, S, S)\n",
    "        \"\"\"\n",
    "        pairs: List[Tuple[int, int, int]] = self._get_responsible_pairs(\n",
    "            boxes, grid_size, self.h, self.w\n",
    "        )\n",
    "        boxes_yolo = self._convert_boxes_to_yolo(\n",
    "            boxes, pairs, grid_size, self.h, self.w\n",
    "        )\n",
    "\n",
    "        tensor = torch.zeros((num_classes + 5, grid_size, grid_size))\n",
    "        for i, (r, c, b) in enumerate(pairs):\n",
    "            tensor[classes[b], r, c] = 1.0\n",
    "            tensor[num_classes, r, c] = 1.0\n",
    "            for j in range(4):\n",
    "                tensor[num_classes + 1 + j, r, c] = boxes_yolo[i][j]\n",
    "        return tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_boxes_to_yolo(\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "        pairs: List[Tuple[int, int, int]],\n",
    "        grid_size: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "    ) -> List[Tuple[float, float, float, float]]:\n",
    "        \"\"\"\n",
    "        Returns a yolo style bbox coordinates for each responsible pair.\n",
    "        \"\"\"\n",
    "        cell_h = h / grid_size\n",
    "        cell_w = w / grid_size\n",
    "\n",
    "        yolo_boxes = []\n",
    "        for r, c, b in pairs:\n",
    "            xmin, ymin, xmax, ymax = boxes[b]\n",
    "\n",
    "            tw = (xmax - xmin) / w\n",
    "            th = (ymax - ymin) / h\n",
    "\n",
    "            mx = (xmax - xmin) / 2\n",
    "            my = (ymax - ymin) / 2\n",
    "            tx = mx / cell_w\n",
    "            ty = my / cell_h\n",
    "\n",
    "            yolo_boxes.append((tx, ty, tw, th))\n",
    "\n",
    "        return yolo_boxes\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_responsible_pairs(\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "        grid_size: int,\n",
    "        h: int,\n",
    "        w: int,\n",
    "    ) -> List[Tuple[int, int, int]]:\n",
    "        \"\"\"\n",
    "        - Find midpoints of all bboxes.\n",
    "        - For all cells, if there's a bbox midpoint in the cell,\n",
    "          that cell and bbox will go in a responsible pair list.\n",
    "        \"\"\"\n",
    "        midpoints = []\n",
    "        for (xmin, ymin, xmax, ymax) in boxes:\n",
    "            x = (xmin + xmax) / 2\n",
    "            y = (ymin + ymax) / 2\n",
    "            midpoints.append((x, y))\n",
    "\n",
    "        cell_h = h / grid_size\n",
    "        cell_w = w / grid_size\n",
    "\n",
    "        pairs = []\n",
    "        for r in range(grid_size):\n",
    "            y1 = r * cell_h\n",
    "            y2 = y1 + cell_h\n",
    "            for c in range(grid_size):\n",
    "                x1 = c * cell_w\n",
    "                x2 = x1 + cell_w\n",
    "                for b, (mx, my) in enumerate(midpoints):\n",
    "                    if x1 < mx < x2 and y1 < my < y2:\n",
    "                        pairs.append((r, c, b))\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_augmentations(h, w, augment: bool):\n",
    "        resizing = [\n",
    "            # A.LongestMaxSize(max_size=WIDTH, always_apply=True),\n",
    "            A.PadIfNeeded(min_height=h, min_width=w, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.RandomCrop(h, w),\n",
    "            # A.Resize(height=HEIGHT, width=WIDTH, always_apply=True),\n",
    "        ]\n",
    "        augmentations = []\n",
    "        if augment:\n",
    "            augmentations = [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ]\n",
    "        compatibility = [\n",
    "            ToTensorV2(always_apply=True),\n",
    "            A.Lambda(image=lambda x, **kwargs: x / 255.0),\n",
    "        ]\n",
    "        return A.Compose(\n",
    "            resizing + augmentations + compatibility,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\", min_visibility=0.05, label_fields=[\"class_labels\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_pre_augmentation(targets: dict) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        This converts the targets compatible with albumentations\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        Output will be of the form:\n",
    "        (\n",
    "            [(xmin, ymin, xmax, ymax), ...],\n",
    "            [3, ...]\n",
    "        )\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        boxes = []\n",
    "        for object in targets[\"annotation\"][\"object\"]:\n",
    "            class_index = VOC_CLASSES.index(object[\"name\"])\n",
    "            classes.append(class_index)\n",
    "\n",
    "            box = object[\"bndbox\"]\n",
    "            box = tuple(int(box[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "            boxes.append(box)\n",
    "\n",
    "        return boxes, classes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 448, 448])\n",
      "torch.Size([2, 25, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"grid_size\": 7,\n",
    "    \"data_path\": \"../data\",\n",
    "    \"batch_size\": 2,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"data_augment\": True,\n",
    "}\n",
    "dataset = VocYoloDataModule(**config)\n",
    "# dataset.prepare_data()\n",
    "dataset.setup()\n",
    "for images, targets in dataset.train_loader():\n",
    "    print(images.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Information about architecture config:\n",
    "- Tuple is structured by (kernel_size, filters, stride, padding)\n",
    "- \"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "- List is structured by tuples and lastly int with number of repeats\n",
    "\"\"\"\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            architecture: List[Union[tuple, str, list]],\n",
    "            in_channels: int,\n",
    "    ):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        for module in architecture:\n",
    "            if type(module) is tuple:\n",
    "                layers.append(self._get_cnn_block(module, in_channels))\n",
    "                in_channels = module[1]\n",
    "            elif module == 'M':\n",
    "                layers.append(nn.MaxPool2d(\n",
    "                    kernel_size=(2, 2),\n",
    "                    stride=(2, 2),\n",
    "                ))\n",
    "            elif type(module) is list:\n",
    "                for i in range(module[-1]):\n",
    "                    for j in range(len(module) - 1):\n",
    "                        layers.append(self._get_cnn_block(module[j], in_channels))\n",
    "                        in_channels = module[j][1]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_cnn_block(module: tuple, in_channels):\n",
    "        kernel_size, filters, stride, padding = module\n",
    "        return CNNBlock(\n",
    "            in_channels,\n",
    "            filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels, split_size, num_boxes, num_classes):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.darknet = SimpleCNN(architecture_config, in_channels)\n",
    "\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.Linear(496, S * S * (C + B * 5)),\n",
    "        )\n",
    "        self.final_shape = (-1, (C + B * 5), S, S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        out = self.fcs(torch.flatten(x, start_dim=1))\n",
    "        out = out.view(self.final_shape)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = YoloV1(in_channels=3, split_size=7, num_boxes=2, num_classes=21)\n",
    "random_batch = torch.rand((2, 3, 448, 448))\n",
    "random_output = model(random_batch)\n",
    "random_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class YoloV1Loss(nn.Module):\n",
    "    def __init__(self, num_boxes: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        Find the responsible cell-bbox pairs.\n",
    "\n",
    "        :param num_boxes: (B)\n",
    "        :param num_classes: (C)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(\n",
    "            self, preds: torch.Tensor, targets: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class YoloV1PL(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.yolo_v1 = YoloV1(\n",
    "            in_channels=3,\n",
    "            split_size=7,\n",
    "            num_boxes=2,\n",
    "            num_classes=21\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.yolo_v1(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('val_loss', loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}