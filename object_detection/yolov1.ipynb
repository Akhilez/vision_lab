{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# YOLO v1\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "Key points:\n",
    "\n",
    "- S x S grid. S = 7\n",
    "- predicts B boxes for each cell. B = 2\n",
    "- Responsible cell:\n",
    "    - the cell that contains bbox midpoint.\n",
    "    - Among B predicted boxes, only the one that has highest IoU will be responsible.\n",
    "- predicts confidence each cell. confidence = IoU\n",
    "- predicts x, y, w, h each cell:\n",
    "    - x, y: they are midpoint coordinates relative to cell origin, h, w.\n",
    "        Meaning, cell h, w are 1, 1, and x, y will be in [0, 1]\n",
    "    - h, w: they are bbox height, width relative to whole image.\n",
    "- predicts C classes each cell.\n",
    "- All are trained only when the cell is responsible for a bbox.\n",
    "- Each cell can only predict 1 object. although it tries to predict B bboxes\n",
    "- Predicted tensor is of shape [S, S, (C + 5B)]\n",
    "- Architecture is simply a CNN followed by a flatten and fully-connected layers.\n",
    "- While inference, multiply C probabilities with predicted confidence.\n",
    "- While inference, apply NMS\n",
    "- All losses are MSE variations.\n",
    "\n",
    "Hyperparams:\n",
    "\n",
    "- leaky relu\n",
    "- batch size 64\n",
    "- epochs 135 (with pre-trained)\n",
    "- momentum 0.9\n",
    "- decay: 0.0005\n",
    "- lr:\n",
    "    - 10^-3 for few epochs.\n",
    "    - 10^-2 for +75 epochs\n",
    "    - 10^-3 for +30 epochs.\n",
    "    - 10^-4 for +30 epochs.\n",
    "- Extensive augmentation:\n",
    "    - Random scaling and translation up to 20%\n",
    "    - randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space.\n",
    "- dropout of 0.5 on last fully-connected\n",
    "\n",
    "Losses:\n",
    "\n",
    "- Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "- Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "- Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "- No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "- Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "confidence = IoU\n",
    "lambda_coord = 5\n",
    "lambda_no_object = 0.5\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (1.4.7)\r\n",
      "Collecting pytorch-lightning\r\n",
      "  Downloading pytorch_lightning-1.4.8-py3-none-any.whl (924 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 924 kB 2.0 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: albumentations in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (1.0.3)\r\n",
      "Requirement already satisfied: wandb in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (0.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (3.10.0.0)\r\n",
      "Requirement already satisfied: torchmetrics>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (0.5.1)\r\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (0.3.1)\r\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (0.18.2)\r\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (4.62.2)\r\n",
      "Requirement already satisfied: torch>=1.6 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (1.9.0.post2)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (1.21.2)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (21.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (5.4.1)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (2.6.0)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pytorch-lightning) (2021.8.1)\r\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from albumentations) (0.18.3)\r\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from albumentations) (1.6.3)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from albumentations) (4.5.3.56)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (7.1.2)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (5.8.0)\r\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (1.16.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (2.26.0)\r\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (3.17.2)\r\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (5.0.2)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (1.1.0)\r\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (2.3)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (3.1.17)\r\n",
      "Requirement already satisfied: pathtools in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (0.1.2)\r\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (1.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (2.8.2)\r\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (3.5.4)\r\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from wandb) (2.1.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.7.4.post0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\r\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.6)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (3.4.3)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.2)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (8.3.2)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2021.8.30)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.13.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.39.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (58.0.4)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.0.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\r\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (5.1.0)\r\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.6.3)\r\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\r\n",
      "Installing collected packages: pytorch-lightning\r\n",
      "  Attempting uninstall: pytorch-lightning\r\n",
      "    Found existing installation: pytorch-lightning 1.4.7\r\n",
      "    Uninstalling pytorch-lightning-1.4.7:\r\n",
      "      Successfully uninstalled pytorch-lightning-1.4.7\r\n",
      "Successfully installed pytorch-lightning-1.4.8\r\n"
     ]
    }
   ],
   "source": [
    "# ! pip install --upgrade pytorch-lightning albumentations wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import AverageMeter, MetricCollection\n",
    "from torchvision.datasets import VOCDetection\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Union, Optional, Tuple, Dict, Any\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class YoloV1Transforms:\n",
    "    def __init__(self, h: int, w: int, augment: bool, num_classes: int, grid_size: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.augment = augment\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.albument_transforms = self._get_augmentations(self.h, self.w, self.augment)\n",
    "\n",
    "    def __call__(self, image, targets: dict):\n",
    "        \"\"\"\n",
    "        The transform function takes in pil image and a dict of target bboxes.\n",
    "        It applies augmentations and returns an image and target tensor of shape (C+5, S, S)\n",
    "        The transform will return image tensor and target tensor.\n",
    "\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        The output target will be a tensor of shape: (C+5, S, S)\n",
    "        :return: Callable function\n",
    "        \"\"\"\n",
    "        boxes, classes = self._transform_pre_augmentation(targets)\n",
    "\n",
    "        transformed = self.albument_transforms(\n",
    "            image=np.array(image),\n",
    "            bboxes=boxes,\n",
    "            class_labels=classes,\n",
    "        )\n",
    "\n",
    "        image = transformed[\"image\"]\n",
    "        boxes = transformed[\"bboxes\"]\n",
    "        classes = transformed[\"class_labels\"]\n",
    "\n",
    "        targets = self.transform_targets_to_yolo(boxes, classes)\n",
    "        return image, targets\n",
    "\n",
    "    def transform_targets_to_yolo(self, boxes, classes) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts (xmin, ymin, xmax, ymax) format to yolo format.\n",
    "\n",
    "        - Get responsible pairs:\n",
    "            - Find midpoints of all bboxes.\n",
    "            - For all cells, if there's a bbox midpoint in the cell,\n",
    "              that cell and bbox will go in a responsible pair list.\n",
    "        - Convert coordinates from (xmin, ymin, ...) to yolo style.\n",
    "        - Put everything in a tensor.\n",
    "\n",
    "        :param boxes: list of tuples of (xmin, ymin, xmax, ymax)\n",
    "        :param classes: list of integers\n",
    "        :return: torch.Tensor of shape (C+5, S, S)\n",
    "        \"\"\"\n",
    "        pairs: List[Tuple[int, int, int]] = self._get_responsible_pairs(boxes)\n",
    "        boxes_yolo = self._convert_boxes_to_yolo(boxes, pairs)\n",
    "\n",
    "        tensor = torch.zeros((self.num_classes + 5, self.grid_size, self.grid_size))\n",
    "        for i, (r, c, b) in enumerate(pairs):\n",
    "            tensor[classes[b], r, c] = 1.0\n",
    "            tensor[self.num_classes, r, c] = 1.0\n",
    "            for j in range(4):\n",
    "                tensor[self.num_classes + 1 + j, r, c] = boxes_yolo[i][j]\n",
    "        return tensor\n",
    "\n",
    "    def transform_targets_from_yolo(self):\n",
    "        # TODO: Finish this\n",
    "        pass\n",
    "\n",
    "    def _convert_boxes_to_yolo(\n",
    "        self,\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "        pairs: List[Tuple[int, int, int]],\n",
    "    ) -> List[Tuple[float, float, float, float]]:\n",
    "        \"\"\"\n",
    "        Returns a yolo style bbox coordinates for each responsible pair.\n",
    "        \"\"\"\n",
    "        cell_h = self.h / self.grid_size\n",
    "        cell_w = self.w / self.grid_size\n",
    "\n",
    "        yolo_boxes = []\n",
    "        for r, c, b in pairs:\n",
    "            xmin, ymin, xmax, ymax = boxes[b]\n",
    "\n",
    "            tw = (xmax - xmin) / self.w\n",
    "            th = (ymax - ymin) / self.h\n",
    "\n",
    "            mx = (xmax - xmin) / 2\n",
    "            my = (ymax - ymin) / 2\n",
    "            tx = mx / cell_w\n",
    "            ty = my / cell_h\n",
    "\n",
    "            yolo_boxes.append((tx, ty, tw, th))\n",
    "\n",
    "        return yolo_boxes\n",
    "\n",
    "    def _get_responsible_pairs(\n",
    "        self,\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "    ) -> List[Tuple[int, int, int]]:\n",
    "        \"\"\"\n",
    "        - Find midpoints of all bboxes.\n",
    "        - For all cells, if there's a bbox midpoint in the cell,\n",
    "          that cell and bbox will go in a responsible pair list.\n",
    "        \"\"\"\n",
    "        midpoints = []\n",
    "        for (xmin, ymin, xmax, ymax) in boxes:\n",
    "            x = (xmin + xmax) / 2\n",
    "            y = (ymin + ymax) / 2\n",
    "            midpoints.append((x, y))\n",
    "\n",
    "        cell_h = self.h / self.grid_size\n",
    "        cell_w = self.w / self.grid_size\n",
    "\n",
    "        pairs = []\n",
    "        for r in range(self.grid_size):\n",
    "            y1 = r * cell_h\n",
    "            y2 = y1 + cell_h\n",
    "            for c in range(self.grid_size):\n",
    "                x1 = c * cell_w\n",
    "                x2 = x1 + cell_w\n",
    "                for b, (mx, my) in enumerate(midpoints):\n",
    "                    if x1 < mx < x2 and y1 < my < y2:\n",
    "                        pairs.append((r, c, b))\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_augmentations(h, w, augment: bool):\n",
    "        def normalize(x, **kwargs):\n",
    "            return x / 255.0\n",
    "\n",
    "\n",
    "        resizing: list = [\n",
    "            # A.LongestMaxSize(max_size=WIDTH, always_apply=True),\n",
    "            A.PadIfNeeded(min_height=h, min_width=w, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.RandomCrop(h, w),\n",
    "            # A.Resize(height=HEIGHT, width=WIDTH, always_apply=True),\n",
    "        ]\n",
    "        compatibility: list = [\n",
    "            ToTensorV2(always_apply=True),\n",
    "            A.Lambda(image=normalize),\n",
    "        ]\n",
    "\n",
    "        augmentations: list = []\n",
    "        if augment:\n",
    "            augmentations = [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ]\n",
    "\n",
    "        return A.Compose(\n",
    "            resizing + augmentations + compatibility,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\", min_visibility=0.05, label_fields=[\"class_labels\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_pre_augmentation(targets: dict) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        This converts the targets compatible with albumentations\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        Output will be of the form:\n",
    "        (\n",
    "            [(xmin, ymin, xmax, ymax), ...],\n",
    "            [3, ...]\n",
    "        )\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        boxes = []\n",
    "        for object in targets[\"annotation\"][\"object\"]:\n",
    "            class_index = VOC_CLASSES.index(object[\"name\"])\n",
    "            classes.append(class_index)\n",
    "\n",
    "            box = object[\"bndbox\"]\n",
    "            box = tuple(int(box[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "            boxes.append(box)\n",
    "\n",
    "        return boxes, classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class PartialVOCDetection(VOCDetection):\n",
    "    def __init__(self, size: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class VocYoloDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int,\n",
    "        batch_size: int,\n",
    "        data_path: str,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        data_augment=False,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = dataloader_num_workers\n",
    "        self.augment = data_augment\n",
    "\n",
    "        self.h = 448\n",
    "        self.w = 448\n",
    "        self.dims = (3, self.h, self.w)\n",
    "        self.num_classes = 20\n",
    "        self.transforms = YoloV1Transforms(\n",
    "            h=self.h,\n",
    "            w=self.w,\n",
    "            augment=self.augment,\n",
    "            num_classes=self.num_classes,\n",
    "            grid_size=self.grid_size,\n",
    "        )\n",
    "\n",
    "        self.dataset_train, self.dataset_val = None, None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"trainval\",\n",
    "            download=True,  # TODO: Makke it True\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.dataset_train = PartialVOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"train\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "            size=20\n",
    "        )\n",
    "        self.dataset_val = PartialVOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"val\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "            size=20\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture: List[Union[tuple, str, list]],\n",
    "        in_channels: int,\n",
    "    ):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        for module in architecture:\n",
    "            if type(module) is tuple:\n",
    "                layers.append(self._get_cnn_block(module, in_channels))\n",
    "                in_channels = module[1]\n",
    "            elif module == \"M\":\n",
    "                layers.append(\n",
    "                    nn.MaxPool2d(\n",
    "                        kernel_size=(2, 2),\n",
    "                        stride=(2, 2),\n",
    "                    )\n",
    "                )\n",
    "            elif type(module) is list:\n",
    "                for i in range(module[-1]):\n",
    "                    for j in range(len(module) - 1):\n",
    "                        layers.append(self._get_cnn_block(module[j], in_channels))\n",
    "                        in_channels = module[j][1]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_cnn_block(module: tuple, in_channels):\n",
    "        kernel_size, filters, stride, padding = module\n",
    "        return CNNBlock(\n",
    "            in_channels,\n",
    "            filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "         LeakyReLU-3         [-1, 64, 224, 224]               0\n",
      "          CNNBlock-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 194, 112, 112]         111,744\n",
      "       BatchNorm2d-7        [-1, 194, 112, 112]             388\n",
      "         LeakyReLU-8        [-1, 194, 112, 112]               0\n",
      "          CNNBlock-9        [-1, 194, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 194, 56, 56]               0\n",
      "           Conv2d-11          [-1, 128, 56, 56]          24,832\n",
      "      BatchNorm2d-12          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-13          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-17          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-18          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-19          [-1, 128, 28, 28]               0\n",
      "           Conv2d-20          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-21          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-22          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-23          [-1, 128, 28, 28]               0\n",
      "           Conv2d-24          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-26          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-27          [-1, 128, 28, 28]               0\n",
      "        MaxPool2d-28          [-1, 128, 14, 14]               0\n",
      "           Conv2d-29          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-30          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-31          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-32          [-1, 128, 14, 14]               0\n",
      "           Conv2d-33          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-34          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-35          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-36          [-1, 128, 14, 14]               0\n",
      "           Conv2d-37             [-1, 64, 7, 7]          73,728\n",
      "      BatchNorm2d-38             [-1, 64, 7, 7]             128\n",
      "        LeakyReLU-39             [-1, 64, 7, 7]               0\n",
      "         CNNBlock-40             [-1, 64, 7, 7]               0\n",
      "           Conv2d-41             [-1, 32, 7, 7]          18,432\n",
      "      BatchNorm2d-42             [-1, 32, 7, 7]              64\n",
      "        LeakyReLU-43             [-1, 32, 7, 7]               0\n",
      "         CNNBlock-44             [-1, 32, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 715,524\n",
      "Trainable params: 715,524\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 216.29\n",
      "Params size (MB): 2.73\n",
      "Estimated Total Size (MB): 221.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Information about architecture config:\n",
    "- Tuple is structured by (kernel_size, filters, stride, padding)\n",
    "- \"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "- List is structured by tuples and lastly int with number of repeats\n",
    "\"\"\"\n",
    "\n",
    "# original_yolo = [\n",
    "#     (7, 64, 2, 3),\n",
    "#     \"M\",\n",
    "#     (3, 192, 1, 1),\n",
    "#     \"M\",\n",
    "#     (1, 128, 1, 0),\n",
    "#     (3, 256, 1, 1),\n",
    "#     (1, 256, 1, 0),\n",
    "#     (3, 512, 1, 1),\n",
    "#     \"M\",\n",
    "#     [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "#     (1, 512, 1, 0),\n",
    "#     (3, 1024, 1, 1),\n",
    "#     \"M\",\n",
    "#     [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "#     (3, 1024, 1, 1),\n",
    "#     (3, 1024, 2, 1),\n",
    "#     (3, 1024, 1, 1),\n",
    "#     (3, 1024, 1, 1),\n",
    "# ]\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),  # 224\n",
    "    \"M\",  # 112\n",
    "    (3, 194, 1, 1),\n",
    "    \"M\",  # 56\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 128, 1, 1),\n",
    "    \"M\",  # 28\n",
    "    [(1, 128, 1, 0), (3, 128, 1, 1), 1],\n",
    "    \"M\", # 14\n",
    "    [(1, 128, 1, 0), (3, 128, 1, 1), 1],\n",
    "    (3, 64, 2, 1),  # 7\n",
    "    (3, 32, 1, 1),\n",
    "]\n",
    "\n",
    "summary(SimpleCNN(architecture_config, in_channels=3), (3, 448, 448))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# mobilenetv2 = torchvision.models.MobileNetV2(num_classes=20)\n",
    "# summary(mobilenetv2.features, (3, 448, 448))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels, split_size, num_boxes, num_classes):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.darknet = SimpleCNN(architecture_config, in_channels)\n",
    "\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * S * S, 128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, S * S * (C + B * 5)),\n",
    "        )\n",
    "        self.final_shape = (-1, (C + B * 5), S, S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        out = self.fcs(torch.flatten(x, start_dim=1))\n",
    "        out = out.view(self.final_shape)\n",
    "        return out\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Losses:\n",
    "\n",
    "    - Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "    - Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "    - Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "    - No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "    - Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "    confidence = IoU\n",
    "    lambda_coord = 5\n",
    "    lambda_no_object = 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boxes: int,\n",
    "        num_classes: int,\n",
    "        lambda_coord: float,\n",
    "        lambda_object_exists: float,\n",
    "        lambda_no_object: float,\n",
    "        lambda_class: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Find the responsible cell-bbox pairs.\n",
    "\n",
    "        :param num_boxes: (B)\n",
    "        :param num_classes: (C)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_object_exists = lambda_object_exists\n",
    "        self.lambda_no_object = lambda_no_object\n",
    "        self.lambda_class = lambda_class\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    def forward(\n",
    "        self, preds: torch.Tensor, targets: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        - Responsible box is the one that has the highest IoU.\n",
    "\n",
    "        IoUs is a 0-1 tensor of shape (batch, B, S, S)\n",
    "        Responsibility is an index tensor of shape (batch, S, S)\n",
    "        object_exists is a 0,1 tensor of shape (batch, 1, S, S)\n",
    "\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return: a dict of all losses.\n",
    "        \"\"\"\n",
    "\n",
    "        ious = self._get_ious(preds.detach(), targets)  # shape: (batch, B, S, S)\n",
    "        responsibility = F.one_hot(\n",
    "            ious.argmax(dim=1), num_classes=self.num_boxes\n",
    "        )  # shape (batch, S, S, B)\n",
    "        object_exists = targets[:, self.num_classes]  # shape: (batch, S, S)\n",
    "        object_not_exists = 1 - object_exists\n",
    "\n",
    "        coords_loss = self._get_coords_loss(\n",
    "            preds, targets, object_exists, responsibility\n",
    "        )\n",
    "        confidence_loss = self._get_confidence_loss(\n",
    "            preds, ious, object_exists, responsibility\n",
    "        )\n",
    "        negative_confidence_loss = self._get_confidence_loss(\n",
    "            preds, ious, object_not_exists, responsibility\n",
    "        )\n",
    "        class_loss = self._get_class_loss(preds, targets, object_exists)\n",
    "\n",
    "        final_loss = (\n",
    "            coords_loss * self.lambda_coord\n",
    "            + confidence_loss * self.lambda_object_exists\n",
    "            + negative_confidence_loss * self.lambda_no_object\n",
    "            + class_loss * self.lambda_class\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": final_loss,\n",
    "            \"loss_coords\": coords_loss.detach(),\n",
    "            \"loss_confidence\": confidence_loss.detach(),\n",
    "            \"loss_confidence_negative\": negative_confidence_loss.detach(),\n",
    "            \"loss_class\": class_loss.detach(),\n",
    "        }\n",
    "\n",
    "    def _get_class_loss(self, preds, targets, object_exists):\n",
    "        c = preds[:, : self.num_classes]  # shape (batch, C, S, S)\n",
    "        c_hat = targets[:, : self.num_classes]  # shape (batch, C, S, S)\n",
    "\n",
    "        c_loss = self.mse(c_hat, c)  # shape (batch, C, S, S)\n",
    "        c_loss = c_loss.sum(dim=1)  # shape (batch, S, S)\n",
    "        c_loss = object_exists * c_loss\n",
    "        c_loss = c_loss.sum(dim=(1, 2)).mean(dim=0)\n",
    "        return c_loss\n",
    "\n",
    "    def _get_confidence_loss(self, preds, ious, object_exists, responsibility):\n",
    "        c_losses = []\n",
    "        for i in range(self.num_boxes):\n",
    "            c = ious[:, i]  # shape (batch, S, S)\n",
    "            c_hat = preds[:, self.num_classes + (i * 5)]  # shape (batch, S, S)\n",
    "\n",
    "            c_loss = self.mse(c_hat, c)\n",
    "            c_loss = object_exists * responsibility[..., i] * c_loss\n",
    "            c_loss = c_loss.sum(dim=(1, 2)).mean(dim=0)\n",
    "            c_losses.append(c_loss)\n",
    "        c_loss = torch.stack(c_losses).sum(dim=0)\n",
    "\n",
    "        return c_loss\n",
    "\n",
    "    def _get_coords_loss(self, preds, targets, object_exists, responsibility):\n",
    "        x = targets[:, self.num_classes + 1]  # shape (batch, S, S)\n",
    "        y = targets[:, self.num_classes + 2]\n",
    "        w = targets[:, self.num_classes + 3]\n",
    "        h = targets[:, self.num_classes + 4]\n",
    "        w_sqrt = torch.sqrt(torch.abs(w))\n",
    "        h_sqrt = torch.sqrt(torch.abs(h))\n",
    "\n",
    "        coords_losses = []  # shape (B,\n",
    "        for i in range(self.num_boxes):\n",
    "            start = self.num_classes + (i * 5)\n",
    "            x_hat = preds[:, start + 1]  # shape (batch, S, S)\n",
    "            y_hat = preds[:, start + 2]\n",
    "            w_hat = preds[:, start + 3]\n",
    "            h_hat = preds[:, start + 4]\n",
    "            w_hat_sqrt = torch.sqrt(torch.abs(w_hat))\n",
    "            h_hat_sqrt = torch.sqrt(torch.abs(h_hat))\n",
    "\n",
    "            xy_loss = self.mse(x_hat, x) + self.mse(y_hat, y)\n",
    "            wh_loss = self.mse(w_hat_sqrt, w_sqrt) + self.mse(h_hat_sqrt, h_sqrt)\n",
    "            coords_loss = object_exists * responsibility[..., i] * (xy_loss + wh_loss)\n",
    "            coords_loss = coords_loss.sum(dim=(1, 2)).mean(\n",
    "                dim=0\n",
    "            )  # average over batch, sum over rest.\n",
    "            coords_losses.append(coords_loss)\n",
    "        coords_loss = torch.stack(coords_losses).sum(dim=0)  # sum over B\n",
    "        return coords_loss\n",
    "\n",
    "    def _get_ious(self, preds, targets) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - When sum(target_[x,y,w,h]) is 0, iou is 0.\n",
    "        - w_cell, h_cell = 1/S\n",
    "        - w_image, h_image = 1\n",
    "\n",
    "        - Get x1, y1, x2, y2 for predicted and target boxes.\n",
    "            - x1 = midpoint_x - (width / 2)\n",
    "        - find box iou\n",
    "\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return: tensor of shape (batch, B, S, S)\n",
    "        \"\"\"\n",
    "\n",
    "        all_coords = []\n",
    "        for i in range(self.num_boxes):\n",
    "            start = self.num_classes + (i * 5) + 1\n",
    "            end = start + 4\n",
    "            coords = preds[:, start:end]\n",
    "            all_coords.append(coords)\n",
    "\n",
    "        coords = targets[:, self.num_classes + 1 :]  # shape: (batch, 1, S, S)\n",
    "        all_coords.append(coords)\n",
    "\n",
    "        all_coords = torch.stack(all_coords)  # shape (B+1, batch, 4, S, S)\n",
    "        all_coords = all_coords.moveaxis(2, 4)  # shape (B+1, batch, S, S, 4)\n",
    "\n",
    "        x = all_coords[..., 0:1]\n",
    "        y = all_coords[..., 1:2]\n",
    "        w = all_coords[..., 2:3]\n",
    "        h = all_coords[..., 3:4]\n",
    "\n",
    "        w_half = w / 2\n",
    "        h_half = h / 2\n",
    "\n",
    "        x1 = x - w_half\n",
    "        y1 = y - h_half\n",
    "        x2 = x + w_half\n",
    "        y2 = y + h_half\n",
    "\n",
    "        # x1 is of shape (B+1, batch, S, S, 1)\n",
    "        coords = torch.cat((x1, y1, x2, y2), dim=4)  # shape (B+1, batch, S, S, 4)\n",
    "\n",
    "        ious = []\n",
    "        for i in range(self.num_boxes):\n",
    "            iou = self.custom_ious(coords[i], coords[-1])  # shape (batch, S, S)\n",
    "            ious.append(iou)\n",
    "        ious = torch.stack(ious)  # shape (B, batch, S, S)\n",
    "        ious = ious.moveaxis(0, 1)  # shape (batch, B, S, S)\n",
    "\n",
    "        return ious\n",
    "\n",
    "    def custom_ious(self, boxes1, boxes2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs 1 to 1 iou\n",
    "        :param boxes1: tensor of shape (*N, 4)\n",
    "        :param boxes2: tensor of shape (*N, 4)\n",
    "        :return: tensor of shape *N\n",
    "        \"\"\"\n",
    "        assert boxes1.shape == boxes2.shape\n",
    "\n",
    "        ax1 = boxes1[..., 0]\n",
    "        ay1 = boxes1[..., 1]\n",
    "        ax2 = boxes1[..., 2]\n",
    "        ay2 = boxes1[..., 3]\n",
    "\n",
    "        bx1 = boxes2[..., 0]\n",
    "        by1 = boxes2[..., 1]\n",
    "        bx2 = boxes2[..., 2]\n",
    "        by2 = boxes2[..., 3]\n",
    "\n",
    "        x1 = self._max(ax1, bx1)\n",
    "        y1 = self._max(ay1, by1)\n",
    "        x2 = self._min(ax2, bx2)\n",
    "        y2 = self._min(ay2, by2)\n",
    "\n",
    "        zeros = torch.zeros_like(x1)\n",
    "        ones = torch.ones_like(x1)\n",
    "\n",
    "        side_x = self._max(zeros, x2 - x1)\n",
    "        side_y = self._max(zeros, y2 - y1)\n",
    "\n",
    "        intersection_area = side_x * side_y\n",
    "\n",
    "        box1_area = (ax2 - ax1) * (ay2 - ay1)\n",
    "        box2_area = (bx2 - bx1) * (by2 - by1)\n",
    "\n",
    "        epsilon = 1e-7\n",
    "        iou = intersection_area / (box1_area + box2_area - intersection_area + epsilon)\n",
    "        iou = self._min(ones, iou)  # shape (*N)\n",
    "        iou[bx2 - bx1 == 0] = 0.0  # Make IoU = 0 when width = 0\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def _max(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simply finds the max off the two tensors.\n",
    "        Shapes of the two tensors has to be same.\n",
    "        \"\"\"\n",
    "        return torch.amax(torch.stack([x, y]), dim=0)\n",
    "\n",
    "    def _min(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simply finds the max off the two tensors.\n",
    "        Shapes of the two tensors has to be same.\n",
    "        \"\"\"\n",
    "        return torch.amin(torch.stack([x, y]), dim=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "\n",
    "class MyMetricCollection(MetricCollection):\n",
    "    def update_each(self, params: dict, **kwargs: Any) -> None:\n",
    "        \"\"\"params is a dict where key is the metric key and the values are tuples of positional arguments.\n",
    "        Keyword arguments (kwargs) will be filtered based on the signature of the individual metric.\n",
    "        \"\"\"\n",
    "        for key, m in self.items(keep_base=True):\n",
    "            if key in params:\n",
    "                args = params[key]\n",
    "                if type(args) is not tuple:\n",
    "                    args = (args,)\n",
    "                m_kwargs = m._filter_kwargs(**kwargs)\n",
    "                m.update(*args, **m_kwargs)\n",
    "\n",
    "    def compute(self):\n",
    "        result = super().compute()\n",
    "        self.reset()\n",
    "        return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1PL(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boxes: int,\n",
    "        num_classes: int,\n",
    "        in_channels: int,\n",
    "        grid_size: int,\n",
    "        lambda_coord: float,\n",
    "        lambda_object_exists: float,\n",
    "        lambda_no_object: float,\n",
    "        lambda_class: float,\n",
    "        **hp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        self.yolo_v1 = YoloV1(\n",
    "            in_channels=in_channels,\n",
    "            split_size=grid_size,\n",
    "            num_boxes=num_boxes,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        self.criterion = YoloV1Loss(\n",
    "            num_boxes=num_boxes,\n",
    "            num_classes=num_classes,\n",
    "            lambda_coord=lambda_coord,\n",
    "            lambda_object_exists=lambda_object_exists,\n",
    "            lambda_no_object=lambda_no_object,\n",
    "            lambda_class=lambda_class,\n",
    "        )\n",
    "\n",
    "        # --- metrics ---\n",
    "        self.metrics_train = MyMetricCollection(\n",
    "            {\n",
    "                \"loss\": AverageMeter(),\n",
    "                \"loss_coords\": AverageMeter(),\n",
    "                \"loss_confidence\": AverageMeter(),\n",
    "                \"loss_confidence_negative\": AverageMeter(),\n",
    "                \"loss_class\": AverageMeter(),\n",
    "            },\n",
    "            prefix=\"train/\",\n",
    "        )\n",
    "        self.metrics_val = self.metrics_train.clone(prefix=\"val/\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.yolo_v1(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hp[\"lr_initial\"])\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.hp[\"lr_decay_every\"],\n",
    "            gamma=self.hp[\"lr_decay_by\"],\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"learning_rate\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, _index):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        losses = self.criterion(preds, targets)\n",
    "        return losses\n",
    "\n",
    "    def training_step_end(self, losses):\n",
    "        self.metrics_train.update_each(losses)\n",
    "        self.log(\"train/loss_step\", losses[\"loss\"], prog_bar=True)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.log_dict(self.metrics_train.compute())\n",
    "\n",
    "    def validation_step(self, batch, _index):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        losses = self.criterion(preds, targets)\n",
    "        return losses\n",
    "\n",
    "    def validation_step_end(self, losses):\n",
    "        self.metrics_val.update_each(losses)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict(self.metrics_val.compute())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "         LeakyReLU-3         [-1, 64, 224, 224]               0\n",
      "          CNNBlock-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 194, 112, 112]         111,744\n",
      "       BatchNorm2d-7        [-1, 194, 112, 112]             388\n",
      "         LeakyReLU-8        [-1, 194, 112, 112]               0\n",
      "          CNNBlock-9        [-1, 194, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 194, 56, 56]               0\n",
      "           Conv2d-11          [-1, 128, 56, 56]          24,832\n",
      "      BatchNorm2d-12          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-13          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-17          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-18          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-19          [-1, 128, 28, 28]               0\n",
      "           Conv2d-20          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-21          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-22          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-23          [-1, 128, 28, 28]               0\n",
      "           Conv2d-24          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-26          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-27          [-1, 128, 28, 28]               0\n",
      "        MaxPool2d-28          [-1, 128, 14, 14]               0\n",
      "           Conv2d-29          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-30          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-31          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-32          [-1, 128, 14, 14]               0\n",
      "           Conv2d-33          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-34          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-35          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-36          [-1, 128, 14, 14]               0\n",
      "           Conv2d-37             [-1, 64, 7, 7]          73,728\n",
      "      BatchNorm2d-38             [-1, 64, 7, 7]             128\n",
      "        LeakyReLU-39             [-1, 64, 7, 7]               0\n",
      "         CNNBlock-40             [-1, 64, 7, 7]               0\n",
      "           Conv2d-41             [-1, 32, 7, 7]          18,432\n",
      "      BatchNorm2d-42             [-1, 32, 7, 7]              64\n",
      "        LeakyReLU-43             [-1, 32, 7, 7]               0\n",
      "         CNNBlock-44             [-1, 32, 7, 7]               0\n",
      "        SimpleCNN-45             [-1, 32, 7, 7]               0\n",
      "          Flatten-46                 [-1, 1568]               0\n",
      "           Linear-47                  [-1, 128]         200,832\n",
      "          Dropout-48                  [-1, 128]               0\n",
      "        LeakyReLU-49                  [-1, 128]               0\n",
      "           Linear-50                 [-1, 1470]         189,630\n",
      "           YoloV1-51             [-1, 30, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 1,105,986\n",
      "Trainable params: 1,105,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 216.34\n",
      "Params size (MB): 4.22\n",
      "Estimated Total Size (MB): 222.85\n",
      "----------------------------------------------------------------\n",
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | yolo_v1       | YoloV1             | 1.1 M \n",
      "1 | criterion     | YoloV1Loss         | 0     \n",
      "2 | metrics_train | MyMetricCollection | 0     \n",
      "3 | metrics_val   | MyMetricCollection | 0     \n",
      "-----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.424     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b0264fa33c3499982384fec25a4ab22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: -1it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20ce0cd9c67444b18e978df2de114ae7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32a7dd49773048688d33d093a81a600f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8ab0291f8d6460bb9a226bac9f77761"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hp = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr_initial\": 0.0001,\n",
    "    \"lr_decay_every\": 20,\n",
    "    \"lr_decay_by\": 0.99,\n",
    "    \"grid_size\": 7,\n",
    "    \"data_augment\": True,\n",
    "    \"num_boxes\": 2,\n",
    "    \"lambda_coord\": 5,\n",
    "    \"lambda_object_exists\": 1,\n",
    "    \"lambda_no_object\": 0.5,\n",
    "    \"lambda_class\": 1,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"output_path\": \"./output\",\n",
    "    \"val_split\": 0.1,\n",
    "    \"data_path\": \"./data\",\n",
    "    \"num_classes\": 20,\n",
    "    \"in_channels\": 3,\n",
    "    \"num_log_images\": 3,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"num_gpus\": 0\n",
    "}\n",
    "\n",
    "data_module = VocYoloDataModule(**config, **hp)\n",
    "model = YoloV1PL(**hp, **config).float()\n",
    "summary(model, (3, 448, 448))\n",
    "# wandb_logger = WandbLogger(project=\"yolo_test\", log_model=False)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=config[\"num_gpus\"],\n",
    "    max_epochs=hp[\"epochs\"],\n",
    "    default_root_dir=config[\"output_path\"],\n",
    "    # logger=wandb_logger,\n",
    ")\n",
    "# wandb_logger.watch(model)\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}