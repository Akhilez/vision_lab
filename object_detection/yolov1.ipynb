{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YOLO v1\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "Key points:\n",
    "\n",
    "- S x S grid. S = 7\n",
    "- predicts B boxes for each cell. B = 2\n",
    "- Responsible cell:\n",
    "    - the cell that contains bbox midpoint.\n",
    "    - Among B predicted boxes, only the one that has highest IoU will be responsible.\n",
    "- predicts confidence each cell. confidence = IoU\n",
    "- predicts x, y, w, h each cell:\n",
    "    - x, y: they are midpoint coordinates relative to cell origin, h, w.\n",
    "        Meaning, cell h, w are 1, 1, and x, y will be in [0, 1]\n",
    "    - h, w: they are bbox height, width relative to whole image.\n",
    "- predicts C classes each cell.\n",
    "- All are trained only when the cell is responsible for a bbox.\n",
    "- Each cell can only predict 1 object. although it tries to predict B bboxes\n",
    "- Predicted tensor is of shape [S, S, (C + 5B)]\n",
    "- Architecture is simply a CNN followed by a flatten and fully-connected layers.\n",
    "- While inference, multiply C probabilities with predicted confidence.\n",
    "- While inference, apply NMS\n",
    "- All losses are MSE variations.\n",
    "\n",
    "Hyperparams:\n",
    "\n",
    "- leaky relu\n",
    "- batch size 64\n",
    "- epochs 135 (with pre-trained)\n",
    "- momentum 0.9\n",
    "- decay: 0.0005\n",
    "- lr:\n",
    "    - 10^-3 for few epochs.\n",
    "    - 10^-2 for +75 epochs\n",
    "    - 10^-3 for +30 epochs.\n",
    "    - 10^-4 for +30 epochs.\n",
    "- Extensive augmentation:\n",
    "    - Random scaling and translation up to 20%\n",
    "    - randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space.\n",
    "- dropout of 0.5 on last fully-connected\n",
    "\n",
    "Losses:\n",
    "\n",
    "- Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "- Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "- Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "- No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "- Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "confidence = IoU\n",
    "lambda_coord = 5\n",
    "lambda_no_object = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning albumentations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.ops\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCDetection\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Union, Optional, Tuple, Dict\n",
    "import pytorch_lightning as pl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1Transforms:\n",
    "    def __init__(self, h: int, w: int, augment: bool, num_classes: int, grid_size: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.augment = augment\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.albument_transforms = self._get_augmentations(self.h, self.w, self.augment)\n",
    "\n",
    "    def __call__(self, image, targets: dict):\n",
    "        \"\"\"\n",
    "        The transform function takes in pil image and a dict of target bboxes.\n",
    "        It applies augmentations and returns an image and target tensor of shape (C+5, S, S)\n",
    "        The transform will return image tensor and target tensor.\n",
    "\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        The output target will be a tensor of shape: (C+5, S, S)\n",
    "        :return: Callable function\n",
    "        \"\"\"\n",
    "        boxes, classes = self._transform_pre_augmentation(targets)\n",
    "\n",
    "        transformed = self.albument_transforms(\n",
    "            image=np.array(image),\n",
    "            bboxes=boxes,\n",
    "            class_labels=classes,\n",
    "        )\n",
    "\n",
    "        image = transformed[\"image\"]\n",
    "        boxes = transformed[\"bboxes\"]\n",
    "        classes = transformed[\"class_labels\"]\n",
    "\n",
    "        targets = self.transform_targets_to_yolo(boxes, classes)\n",
    "        return image, targets\n",
    "\n",
    "    def transform_targets_to_yolo(self, boxes, classes) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts (xmin, ymin, xmax, ymax) format to yolo format.\n",
    "\n",
    "        - Get responsible pairs:\n",
    "            - Find midpoints of all bboxes.\n",
    "            - For all cells, if there's a bbox midpoint in the cell,\n",
    "              that cell and bbox will go in a responsible pair list.\n",
    "        - Convert coordinates from (xmin, ymin, ...) to yolo style.\n",
    "        - Put everything in a tensor.\n",
    "\n",
    "        :param boxes: list of tuples of (xmin, ymin, xmax, ymax)\n",
    "        :param classes: list of integers\n",
    "        :return: torch.Tensor of shape (C+5, S, S)\n",
    "        \"\"\"\n",
    "        pairs: List[Tuple[int, int, int]] = self._get_responsible_pairs(boxes)\n",
    "        boxes_yolo = self._convert_boxes_to_yolo(boxes, pairs)\n",
    "\n",
    "        tensor = torch.zeros((self.num_classes + 5, self.grid_size, self.grid_size))\n",
    "        for i, (r, c, b) in enumerate(pairs):\n",
    "            tensor[classes[b], r, c] = 1.0\n",
    "            tensor[self.num_classes, r, c] = 1.0\n",
    "            for j in range(4):\n",
    "                tensor[self.num_classes + 1 + j, r, c] = boxes_yolo[i][j]\n",
    "        return tensor\n",
    "\n",
    "    def transform_targets_from_yolo(self):\n",
    "        # TODO: Finish this\n",
    "        pass\n",
    "\n",
    "    def _convert_boxes_to_yolo(\n",
    "        self,\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "        pairs: List[Tuple[int, int, int]],\n",
    "    ) -> List[Tuple[float, float, float, float]]:\n",
    "        \"\"\"\n",
    "        Returns a yolo style bbox coordinates for each responsible pair.\n",
    "        \"\"\"\n",
    "        cell_h = self.h / self.grid_size\n",
    "        cell_w = self.w / self.grid_size\n",
    "\n",
    "        yolo_boxes = []\n",
    "        for r, c, b in pairs:\n",
    "            xmin, ymin, xmax, ymax = boxes[b]\n",
    "\n",
    "            tw = (xmax - xmin) / self.w\n",
    "            th = (ymax - ymin) / self.h\n",
    "\n",
    "            mx = (xmax - xmin) / 2\n",
    "            my = (ymax - ymin) / 2\n",
    "            tx = mx / cell_w\n",
    "            ty = my / cell_h\n",
    "\n",
    "            yolo_boxes.append((tx, ty, tw, th))\n",
    "\n",
    "        return yolo_boxes\n",
    "\n",
    "    def _get_responsible_pairs(\n",
    "        self,\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "    ) -> List[Tuple[int, int, int]]:\n",
    "        \"\"\"\n",
    "        - Find midpoints of all bboxes.\n",
    "        - For all cells, if there's a bbox midpoint in the cell,\n",
    "          that cell and bbox will go in a responsible pair list.\n",
    "        \"\"\"\n",
    "        midpoints = []\n",
    "        for (xmin, ymin, xmax, ymax) in boxes:\n",
    "            x = (xmin + xmax) / 2\n",
    "            y = (ymin + ymax) / 2\n",
    "            midpoints.append((x, y))\n",
    "\n",
    "        cell_h = self.h / self.grid_size\n",
    "        cell_w = self.w / self.grid_size\n",
    "\n",
    "        pairs = []\n",
    "        for r in range(self.grid_size):\n",
    "            y1 = r * cell_h\n",
    "            y2 = y1 + cell_h\n",
    "            for c in range(self.grid_size):\n",
    "                x1 = c * cell_w\n",
    "                x2 = x1 + cell_w\n",
    "                for b, (mx, my) in enumerate(midpoints):\n",
    "                    if x1 < mx < x2 and y1 < my < y2:\n",
    "                        pairs.append((r, c, b))\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_augmentations(h, w, augment: bool):\n",
    "        resizing: list = [\n",
    "            # A.LongestMaxSize(max_size=WIDTH, always_apply=True),\n",
    "            A.PadIfNeeded(min_height=h, min_width=w, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.RandomCrop(h, w),\n",
    "            # A.Resize(height=HEIGHT, width=WIDTH, always_apply=True),\n",
    "        ]\n",
    "        compatibility: list = [\n",
    "            ToTensorV2(always_apply=True),\n",
    "            A.Lambda(image=lambda x, **kwargs: x / 255.0),\n",
    "        ]\n",
    "\n",
    "        augmentations: list = []\n",
    "        if augment:\n",
    "            augmentations = [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ]\n",
    "\n",
    "        return A.Compose(\n",
    "            resizing + augmentations + compatibility,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\", min_visibility=0.05, label_fields=[\"class_labels\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_pre_augmentation(targets: dict) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        This converts the targets compatible with albumentations\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        Output will be of the form:\n",
    "        (\n",
    "            [(xmin, ymin, xmax, ymax), ...],\n",
    "            [3, ...]\n",
    "        )\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        boxes = []\n",
    "        for object in targets[\"annotation\"][\"object\"]:\n",
    "            class_index = VOC_CLASSES.index(object[\"name\"])\n",
    "            classes.append(class_index)\n",
    "\n",
    "            box = object[\"bndbox\"]\n",
    "            box = tuple(int(box[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "            boxes.append(box)\n",
    "\n",
    "        return boxes, classes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class VocYoloDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int,\n",
    "        batch_size: int,\n",
    "        data_path: str,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        data_augment=False,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = dataloader_num_workers\n",
    "        self.augment = data_augment\n",
    "\n",
    "        self.h = 448\n",
    "        self.w = 448\n",
    "        self.dims = (3, self.h, self.w)\n",
    "        self.num_classes = 20\n",
    "        self.transforms = YoloV1Transforms(\n",
    "            h=self.h,\n",
    "            w=self.w,\n",
    "            augment=self.augment,\n",
    "            num_classes=self.num_classes,\n",
    "            grid_size=self.grid_size,\n",
    "        )\n",
    "\n",
    "        self.dataset_train, self.dataset_val = None, None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"trainval\",\n",
    "            download=False,  # TODO: make this True\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.dataset_train = VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"train\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "        )\n",
    "        self.dataset_val = VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"val\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"grid_size\": 7,\n",
    "    \"data_path\": \"../data\",\n",
    "    \"batch_size\": 2,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"data_augment\": True,\n",
    "}\n",
    "dataset = VocYoloDataModule(**config)\n",
    "# dataset.prepare_data()\n",
    "dataset.setup()\n",
    "for images, targets in dataset.train_dataloader():\n",
    "    print(images.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Information about architecture config:\n",
    "- Tuple is structured by (kernel_size, filters, stride, padding)\n",
    "- \"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "- List is structured by tuples and lastly int with number of repeats\n",
    "\"\"\"\n",
    "\n",
    "# architecture_config = [\n",
    "#     (7, 64, 2, 3),\n",
    "#     \"M\",\n",
    "#     (3, 192, 1, 1),\n",
    "#     \"M\",\n",
    "#     (1, 128, 1, 0),\n",
    "#     (3, 256, 1, 1),\n",
    "#     (1, 256, 1, 0),\n",
    "#     (3, 512, 1, 1),\n",
    "#     \"M\",\n",
    "#     [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "#     (1, 512, 1, 0),\n",
    "#     (3, 1024, 1, 1),\n",
    "#     \"M\",\n",
    "#     [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "#     (3, 1024, 1, 1),\n",
    "#     (3, 1024, 2, 1),\n",
    "#     (3, 1024, 1, 1),\n",
    "#     (3, 1024, 1, 1),\n",
    "# ]\n",
    "architecture_config = [\n",
    "    (7, 2, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 2, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 2, 1, 0),\n",
    "    (3, 2, 1, 1),\n",
    "    (1, 2, 1, 0),\n",
    "    (3, 2, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 2, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 2, 1, 0),\n",
    "    (3, 2, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 2, 1, 0), (3, 2, 1, 1), 2],\n",
    "    (3, 2, 1, 1),\n",
    "    (3, 2, 2, 1),\n",
    "    (3, 2, 1, 1),\n",
    "    (3, 2, 1, 1),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture: List[Union[tuple, str, list]],\n",
    "        in_channels: int,\n",
    "    ):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        for module in architecture:\n",
    "            if type(module) is tuple:\n",
    "                layers.append(self._get_cnn_block(module, in_channels))\n",
    "                in_channels = module[1]\n",
    "            elif module == \"M\":\n",
    "                layers.append(\n",
    "                    nn.MaxPool2d(\n",
    "                        kernel_size=(2, 2),\n",
    "                        stride=(2, 2),\n",
    "                    )\n",
    "                )\n",
    "            elif type(module) is list:\n",
    "                for i in range(module[-1]):\n",
    "                    for j in range(len(module) - 1):\n",
    "                        layers.append(self._get_cnn_block(module[j], in_channels))\n",
    "                        in_channels = module[j][1]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_cnn_block(module: tuple, in_channels):\n",
    "        kernel_size, filters, stride, padding = module\n",
    "        return CNNBlock(\n",
    "            in_channels,\n",
    "            filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels, split_size, num_boxes, num_classes):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.darknet = SimpleCNN(architecture_config, in_channels)\n",
    "\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * S * S, 496),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),\n",
    "        )\n",
    "        self.final_shape = (-1, (C + B * 5), S, S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        out = self.fcs(torch.flatten(x, start_dim=1))\n",
    "        out = out.view(self.final_shape)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = YoloV1(in_channels=3, split_size=7, num_boxes=2, num_classes=21)\n",
    "random_batch = torch.rand((2, 3, 448, 448))\n",
    "random_output = model(random_batch)\n",
    "print(random_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Losses:\n",
    "\n",
    "    - Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "    - Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "    - Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "    - No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "    - Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "    confidence = IoU\n",
    "    lambda_coord = 5\n",
    "    lambda_no_object = 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boxes: int,\n",
    "        num_classes: int,\n",
    "        lambda_coord: float,\n",
    "        lambda_object_exists: float,\n",
    "        lambda_no_object: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Find the responsible cell-bbox pairs.\n",
    "\n",
    "        :param num_boxes: (B)\n",
    "        :param num_classes: (C)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_object_exists = lambda_object_exists\n",
    "        self.lambda_no_object = lambda_no_object\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(\n",
    "        self, preds: torch.Tensor, targets: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        - Responsible box is the one that has the highest IoU.\n",
    "\n",
    "        IoUs is a 0-1 tensor of shape (batch, B, S, S)\n",
    "        Responsibility is an index tensor of shape (batch, S, S)\n",
    "        object_exists is a 0,1 tensor of shape (batch, 1, S, S)\n",
    "\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return: a dict of all losses.\n",
    "        \"\"\"\n",
    "\n",
    "        ious = self._get_ious(preds, targets)  # shape: (batch, B, S, S)\n",
    "        responsibility = F.one_hot(ious.argmax(dim=1), num_classes=self.num_boxes)  # shape (batch, B, S, S)\n",
    "        object_exists = targets[:, self.num_classes]  # shape: (batch, S, S)\n",
    "        object_not_exists = 1 - object_exists\n",
    "\n",
    "        coords_loss = self._get_coords_loss(preds, targets, object_exists, responsibility)\n",
    "        confidence_loss = self._get_confidence_loss(preds, ious, object_exists, responsibility)\n",
    "        negative_confidence_loss = self._get_confidence_loss(preds, ious, object_not_exists, responsibility)\n",
    "\n",
    "\n",
    "\n",
    "        # ----------- confidence ------------\n",
    "\n",
    "    def _get_confidence_loss(self, preds, ious, object_exists, responsibility):\n",
    "        c_losses = []\n",
    "        for i in range(self.num_boxes):\n",
    "            c = ious[:, i]  # shape (batch, S, S)\n",
    "            c_hat = preds[:, self.num_classes + (i * 5)]  # shape (batch, S, S)\n",
    "\n",
    "            c_loss = self.mse(c_hat, c)\n",
    "            c_loss = object_exists * responsibility[i] * c_loss\n",
    "            c_loss = c_loss.sum(dim=(1, 2)).mean(dim=0)\n",
    "            c_losses.append(c_loss)\n",
    "        c_loss = torch.stack(c_losses).sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        return c_loss\n",
    "\n",
    "    def _get_coords_loss(self, preds, targets, object_exists, responsibility):\n",
    "        x = targets[:, self.num_classes + 1]  # shape (batch, S, S)\n",
    "        y = targets[:, self.num_classes + 2]\n",
    "        w = targets[:, self.num_classes + 3]\n",
    "        h = targets[:, self.num_classes + 4]\n",
    "        w_sqrt = torch.sqrt(w)\n",
    "        h_sqrt = torch.sqrt(h)\n",
    "\n",
    "        coords_losses = []  # shape (B,\n",
    "        for i in range(self.num_boxes):\n",
    "            start = self.num_classes + (i * 5)\n",
    "            x_hat = preds[:, start + 1]  # shape (batch, S, S)\n",
    "            y_hat = preds[:, start + 2]\n",
    "            w_hat = preds[:, start + 3]\n",
    "            h_hat = preds[:, start + 4]\n",
    "            w_hat_sqrt = torch.sqrt(w_hat)\n",
    "            h_hat_sqrt = torch.sqrt(h_hat)\n",
    "\n",
    "            xy_loss = self.mse(x_hat, x) + self.mse(y_hat, y)\n",
    "            wh_loss = self.mse(w_hat_sqrt, w_sqrt) + self.mse(h_hat_sqrt, h_sqrt)\n",
    "            coords_loss = object_exists * responsibility[i] * (xy_loss + wh_loss)\n",
    "            coords_loss = coords_loss.sum(dim=(1, 2)).mean(dim=0)  # average over batch, sum over rest.\n",
    "            coords_losses.append(coords_loss)\n",
    "        coords_loss = torch.stack(coords_losses).sum(dim=0)  # sum over B\n",
    "        return coords_loss\n",
    "\n",
    "\n",
    "    def _get_ious(self, preds, targets) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - When sum(target_[x,y,w,h]) is 0, iou is 0.\n",
    "        - w_cell, h_cell = 1/S\n",
    "        - w_image, h_image = 1\n",
    "\n",
    "        - Get x1, y1, x2, y2 for predicted and target boxes.\n",
    "            - x1 = midpoint_x - (width / 2)\n",
    "        - find box iou\n",
    "\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return: tensor of shape (batch, B, S, S)\n",
    "        \"\"\"\n",
    "\n",
    "        all_coords = []\n",
    "        for i in range(self.num_boxes):\n",
    "            start = self.num_classes + (i * 5) + 1\n",
    "            end = start + 4\n",
    "            coords = preds[:, start:end]\n",
    "            all_coords.append(coords)\n",
    "\n",
    "        coords = targets[:, self.num_classes + 1 :]  # shape: (batch, 1, S, S)\n",
    "        all_coords.append(coords)\n",
    "\n",
    "        all_coords = torch.stack(all_coords)  # shape (B+1, batch, 4, S, S)\n",
    "        all_coords = all_coords.moveaxis(2, 4)  # shape (B+1, batch, S, S, 4)\n",
    "\n",
    "        x = all_coords[..., 0:1]\n",
    "        y = all_coords[..., 1:2]\n",
    "        w = all_coords[..., 2:3]\n",
    "        h = all_coords[..., 3:4]\n",
    "\n",
    "        w_half = w / 2\n",
    "        h_half = h / 2\n",
    "\n",
    "        x1 = x - w_half\n",
    "        y1 = y - h_half\n",
    "        x2 = x + w_half\n",
    "        y2 = y + h_half\n",
    "\n",
    "        # x1 is of shape (B+1, batch, S, S, 1)\n",
    "        coords = torch.cat((x1, y1, x2, y2), dim=4)  # shape (B+1, batch, S, S, 4)\n",
    "\n",
    "        ious = []\n",
    "        for i in range(self.num_boxes):\n",
    "            iou = self.custom_ious(coords[i], coords[-1])  # shape (batch, S, S)\n",
    "            ious.append(iou)\n",
    "        ious = torch.stack(ious)  # shape (B, batch, S, S)\n",
    "        ious = ious.moveaxis(0, 1)  # shape (batch, B, S, S)\n",
    "\n",
    "        return ious\n",
    "\n",
    "    def custom_ious(self, boxes1, boxes2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs 1 to 1 iou\n",
    "        :param boxes1: tensor of shape (*N, 4)\n",
    "        :param boxes2: tensor of shape (*N, 4)\n",
    "        :return: tensor of shape *N\n",
    "        \"\"\"\n",
    "        assert boxes1.shape == boxes2.shape\n",
    "\n",
    "        ax1 = boxes1[..., 0]\n",
    "        ay1 = boxes1[..., 1]\n",
    "        ax2 = boxes1[..., 2]\n",
    "        ay2 = boxes1[..., 3]\n",
    "\n",
    "        bx1 = boxes2[..., 0]\n",
    "        by1 = boxes2[..., 1]\n",
    "        bx2 = boxes2[..., 2]\n",
    "        by2 = boxes2[..., 3]\n",
    "\n",
    "        x1 = self._max(ax1, bx1)\n",
    "        y1 = self._max(ay1, by1)\n",
    "        x2 = self._min(ax2, bx2)\n",
    "        y2 = self._min(ay2, by2)\n",
    "\n",
    "        zeros = torch.zeros_like(x1)\n",
    "        ones = torch.ones_like(x1)\n",
    "\n",
    "        side_x = self._max(zeros, x2 - x1)\n",
    "        side_y = self._max(zeros, y2 - y1)\n",
    "\n",
    "        intersection_area = side_x * side_y\n",
    "\n",
    "        box1_area = (ax2 - ax1) * (ay2 - ay1)\n",
    "        box2_area = (bx2 - bx1) * (by2 - by1)\n",
    "\n",
    "        epsilon = 1e-7\n",
    "        iou = intersection_area / (box1_area + box2_area - intersection_area + epsilon)\n",
    "        iou = self._min(ones, iou)  # shape (*N)\n",
    "        iou[bx2 - bx1 == 0] = 0.0  # Make IoU = 0 when width = 0\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def _max(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simply finds the max off the two tensors.\n",
    "        Shapes of the two tensors has to be same.\n",
    "        \"\"\"\n",
    "        return torch.amax(torch.stack([x, y]), dim=0)\n",
    "\n",
    "    def _min(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simply finds the max off the two tensors.\n",
    "        Shapes of the two tensors has to be same.\n",
    "        \"\"\"\n",
    "        return torch.amin(torch.stack([x, y]), dim=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1PL(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boxes: int,\n",
    "        num_classes: int,\n",
    "        in_channels: int,\n",
    "        grid_size: int,\n",
    "        lambda_coord: float,\n",
    "        lambda_object_exists: float,\n",
    "        lambda_no_object: float,\n",
    "        **hp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        self.yolo_v1 = YoloV1(\n",
    "            in_channels=in_channels,\n",
    "            split_size=grid_size,\n",
    "            num_boxes=num_boxes,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        self.criterion = YoloV1Loss(\n",
    "            num_boxes=num_boxes,\n",
    "            num_classes=num_classes,\n",
    "            lambda_coord=lambda_coord,\n",
    "            lambda_object_exists=lambda_object_exists,\n",
    "            lambda_no_object=lambda_no_object,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.yolo_v1(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hp[\"lr_initial\"])\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.hp[\"lr_decay_every\"],\n",
    "            gamma=self.hp[\"lr_decay_by\"],\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"learning_rate\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, _index):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _index):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        loss = self.criterion(preds, targets)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp = {\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr_initial\": 0.0001,\n",
    "    \"lr_decay_every\": 20,\n",
    "    \"lr_decay_by\": 0.99,\n",
    "    \"grid_size\": 7,\n",
    "    \"data_augment\": True,\n",
    "    \"num_boxes\": 2,\n",
    "    \"lambda_coord\": 5,\n",
    "    \"lambda_object_exists\": 1,\n",
    "    \"lambda_no_object\": 0.5,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"output_path\": \"./output\",\n",
    "    \"val_split\": 0.1,\n",
    "    \"data_path\": \"./data\",\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"num_classes\": 20,\n",
    "    \"in_channels\": 3,\n",
    "}\n",
    "\n",
    "data_module = VocYoloDataModule(**config, **hp)\n",
    "model = YoloV1PL(**hp, **config).float()\n",
    "# wandb_logger = WandbLogger(project=\"yolo_test\", log_model=True)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    max_epochs=hp[\"epochs\"],\n",
    "    default_root_dir=config[\"output_path\"],\n",
    "    # logger=wandb_logger,\n",
    ")\n",
    "# wandb_logger.watch(model)\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}