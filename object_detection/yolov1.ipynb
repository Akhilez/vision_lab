{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# YOLO v1\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "Key points:\n",
    "\n",
    "- S x S grid. S = 7\n",
    "- predicts B boxes for each cell. B = 2\n",
    "- Responsible cell:\n",
    "    - the cell that contains bbox midpoint.\n",
    "    - Among B predicted boxes, only the one that has highest IoU will be responsible.\n",
    "- predicts confidence each cell. confidence = IoU\n",
    "- predicts x, y, w, h each cell:\n",
    "    - x, y: they are midpoint coordinates relative to cell origin, h, w.\n",
    "        Meaning, cell h, w are 1, 1, and x, y will be in [0, 1]\n",
    "    - h, w: they are bbox height, width relative to whole image.\n",
    "- predicts C classes each cell.\n",
    "- All are trained only when the cell is responsible for a bbox.\n",
    "- Each cell can only predict 1 object. although it tries to predict B bboxes\n",
    "- Predicted tensor is of shape [S, S, (C + 5B)]\n",
    "- Architecture is simply a CNN followed by a flatten and fully-connected layers.\n",
    "- While inference, multiply C probabilities with predicted confidence.\n",
    "- While inference, apply NMS\n",
    "- All losses are MSE variations.\n",
    "\n",
    "Hyperparams:\n",
    "\n",
    "- leaky relu\n",
    "- batch size 64\n",
    "- epochs 135 (with pre-trained)\n",
    "- momentum 0.9\n",
    "- decay: 0.0005\n",
    "- lr:\n",
    "    - 10^-3 for few epochs.\n",
    "    - 10^-2 for +75 epochs\n",
    "    - 10^-3 for +30 epochs.\n",
    "    - 10^-4 for +30 epochs.\n",
    "- Extensive augmentation:\n",
    "    - Random scaling and translation up to 20%\n",
    "    - randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space.\n",
    "- dropout of 0.5 on last fully-connected\n",
    "\n",
    "Losses:\n",
    "\n",
    "- Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "- Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "- Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "- No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "- Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "confidence = IoU\n",
    "lambda_coord = 5\n",
    "lambda_no_object = 0.5\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pytorch-lightning albumentations wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import AverageMeter, MetricCollection\n",
    "from torchvision.datasets import VOCDetection\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Union, Optional, Tuple, Dict, Any\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class YoloV1Transforms:\n",
    "    def __init__(self, h: int, w: int, augment: bool, num_classes: int, grid_size: int):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.augment = augment\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.albument_transforms = self._get_augmentations(self.h, self.w, self.augment)\n",
    "\n",
    "    def __call__(self, image, targets: dict):\n",
    "        \"\"\"\n",
    "        The transform function takes in pil image and a dict of target bboxes.\n",
    "        It applies augmentations and returns an image and target tensor of shape (C+5, S, S)\n",
    "        The transform will return image tensor and target tensor.\n",
    "\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        The output target will be a tensor of shape: (C+5, S, S)\n",
    "        :return: Callable function\n",
    "        \"\"\"\n",
    "        boxes, classes = self._transform_pre_augmentation(targets)\n",
    "\n",
    "        transformed = self.albument_transforms(\n",
    "            image=np.array(image),\n",
    "            bboxes=boxes,\n",
    "            class_labels=classes,\n",
    "        )\n",
    "\n",
    "        image = transformed[\"image\"]\n",
    "        boxes = transformed[\"bboxes\"]\n",
    "        classes = transformed[\"class_labels\"]\n",
    "\n",
    "        targets = self.transform_targets_to_yolo(boxes, classes)\n",
    "        return image, targets\n",
    "\n",
    "    def transform_targets_to_yolo(self, boxes, classes) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts (xmin, ymin, xmax, ymax) format to yolo format.\n",
    "\n",
    "        - Get responsible pairs:\n",
    "            - Find midpoints of all bboxes.\n",
    "            - For all cells, if there's a bbox midpoint in the cell,\n",
    "              that cell and bbox will go in a responsible pair list.\n",
    "        - Convert coordinates from (xmin, ymin, ...) to yolo style.\n",
    "        - Put everything in a tensor.\n",
    "\n",
    "        :param boxes: list of tuples of (xmin, ymin, xmax, ymax)\n",
    "        :param classes: list of integers\n",
    "        :return: torch.Tensor of shape (C+5, S, S)\n",
    "        \"\"\"\n",
    "        pairs: List[Tuple[int, int, int]] = self._get_responsible_pairs(boxes)\n",
    "        boxes_yolo = self._convert_boxes_to_yolo(boxes, pairs)\n",
    "\n",
    "        tensor = torch.zeros((self.num_classes + 5, self.grid_size, self.grid_size))\n",
    "        for i, (r, c, b) in enumerate(pairs):\n",
    "            tensor[classes[b], r, c] = 1.0\n",
    "            tensor[self.num_classes, r, c] = 1.0\n",
    "            for j in range(4):\n",
    "                tensor[self.num_classes + 1 + j, r, c] = boxes_yolo[i][j]\n",
    "        return tensor\n",
    "\n",
    "    def transform_targets_from_yolo(self):\n",
    "        # TODO: Finish this\n",
    "        pass\n",
    "\n",
    "    def _convert_boxes_to_yolo(\n",
    "        self,\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "        pairs: List[Tuple[int, int, int]],\n",
    "    ) -> List[Tuple[float, float, float, float]]:\n",
    "        \"\"\"\n",
    "        Returns a yolo style bbox coordinates for each responsible pair.\n",
    "        \"\"\"\n",
    "        cell_h = self.h / self.grid_size\n",
    "        cell_w = self.w / self.grid_size\n",
    "\n",
    "        yolo_boxes = []\n",
    "        for r, c, b in pairs:\n",
    "            xmin, ymin, xmax, ymax = boxes[b]\n",
    "\n",
    "            tw = (xmax - xmin) / self.w\n",
    "            th = (ymax - ymin) / self.h\n",
    "\n",
    "            mx = (xmax - xmin) / 2\n",
    "            my = (ymax - ymin) / 2\n",
    "            tx = mx / cell_w\n",
    "            ty = my / cell_h\n",
    "\n",
    "            yolo_boxes.append((tx, ty, tw, th))\n",
    "\n",
    "        return yolo_boxes\n",
    "\n",
    "    def _get_responsible_pairs(\n",
    "        self,\n",
    "        boxes: List[Tuple[int, int, int, int]],\n",
    "    ) -> List[Tuple[int, int, int]]:\n",
    "        \"\"\"\n",
    "        - Find midpoints of all bboxes.\n",
    "        - For all cells, if there's a bbox midpoint in the cell,\n",
    "          that cell and bbox will go in a responsible pair list.\n",
    "        \"\"\"\n",
    "        midpoints = []\n",
    "        for (xmin, ymin, xmax, ymax) in boxes:\n",
    "            x = (xmin + xmax) / 2\n",
    "            y = (ymin + ymax) / 2\n",
    "            midpoints.append((x, y))\n",
    "\n",
    "        cell_h = self.h / self.grid_size\n",
    "        cell_w = self.w / self.grid_size\n",
    "\n",
    "        pairs = []\n",
    "        for r in range(self.grid_size):\n",
    "            y1 = r * cell_h\n",
    "            y2 = y1 + cell_h\n",
    "            for c in range(self.grid_size):\n",
    "                x1 = c * cell_w\n",
    "                x2 = x1 + cell_w\n",
    "                for b, (mx, my) in enumerate(midpoints):\n",
    "                    if x1 < mx < x2 and y1 < my < y2:\n",
    "                        pairs.append((r, c, b))\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_augmentations(h, w, augment: bool):\n",
    "        def normalize(x, **kwargs):\n",
    "            return x / 255.0\n",
    "\n",
    "\n",
    "        resizing: list = [\n",
    "            # A.LongestMaxSize(max_size=WIDTH, always_apply=True),\n",
    "            A.PadIfNeeded(min_height=h, min_width=w, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.RandomCrop(h, w),\n",
    "            # A.Resize(height=HEIGHT, width=WIDTH, always_apply=True),\n",
    "        ]\n",
    "        compatibility: list = [\n",
    "            ToTensorV2(always_apply=True),\n",
    "            A.Lambda(image=normalize),\n",
    "        ]\n",
    "\n",
    "        augmentations: list = []\n",
    "        if augment:\n",
    "            augmentations = [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ]\n",
    "\n",
    "        return A.Compose(\n",
    "            resizing + augmentations + compatibility,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\", min_visibility=0.05, label_fields=[\"class_labels\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_pre_augmentation(targets: dict) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        This converts the targets compatible with albumentations\n",
    "        The target is of the shape excluding unrelated info:\n",
    "        ```\n",
    "        annotation:\n",
    "          object:\n",
    "            - name: bicycle\n",
    "              bndbox:\n",
    "                xmax: 471\n",
    "                xmin: 54\n",
    "                ymax: 336\n",
    "                ymin: 39\n",
    "        ```\n",
    "        Output will be of the form:\n",
    "        (\n",
    "            [(xmin, ymin, xmax, ymax), ...],\n",
    "            [3, ...]\n",
    "        )\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        boxes = []\n",
    "        for object in targets[\"annotation\"][\"object\"]:\n",
    "            class_index = VOC_CLASSES.index(object[\"name\"])\n",
    "            classes.append(class_index)\n",
    "\n",
    "            box = object[\"bndbox\"]\n",
    "            box = tuple(int(box[key]) for key in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "            boxes.append(box)\n",
    "\n",
    "        return boxes, classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PartialVOCDetection(VOCDetection):\n",
    "    def __init__(self, size: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class VocYoloDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int,\n",
    "        batch_size: int,\n",
    "        data_path: str,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        data_augment=False,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = dataloader_num_workers\n",
    "        self.augment = data_augment\n",
    "\n",
    "        self.h = 448\n",
    "        self.w = 448\n",
    "        self.dims = (3, self.h, self.w)\n",
    "        self.num_classes = 20\n",
    "        self.transforms = YoloV1Transforms(\n",
    "            h=self.h,\n",
    "            w=self.w,\n",
    "            augment=self.augment,\n",
    "            num_classes=self.num_classes,\n",
    "            grid_size=self.grid_size,\n",
    "        )\n",
    "\n",
    "        self.dataset_train, self.dataset_val = None, None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        VOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"trainval\",\n",
    "            download=True,  # TODO: Makke it True\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.dataset_train = PartialVOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"train\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "            size=20\n",
    "        )\n",
    "        self.dataset_val = PartialVOCDetection(\n",
    "            root=self.data_path,\n",
    "            year=\"2012\",\n",
    "            image_set=\"val\",\n",
    "            download=False,\n",
    "            transforms=self.transforms,\n",
    "            size=20\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture: List[Union[tuple, str, list]],\n",
    "        in_channels: int,\n",
    "    ):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        for module in architecture:\n",
    "            if type(module) is tuple:\n",
    "                layers.append(self._get_cnn_block(module, in_channels))\n",
    "                in_channels = module[1]\n",
    "            elif module == \"M\":\n",
    "                layers.append(\n",
    "                    nn.MaxPool2d(\n",
    "                        kernel_size=(2, 2),\n",
    "                        stride=(2, 2),\n",
    "                    )\n",
    "                )\n",
    "            elif type(module) is list:\n",
    "                for i in range(module[-1]):\n",
    "                    for j in range(len(module) - 1):\n",
    "                        layers.append(self._get_cnn_block(module[j], in_channels))\n",
    "                        in_channels = module[j][1]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_cnn_block(module: tuple, in_channels):\n",
    "        kernel_size, filters, stride, padding = module\n",
    "        return CNNBlock(\n",
    "            in_channels,\n",
    "            filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "         LeakyReLU-3         [-1, 64, 224, 224]               0\n",
      "          CNNBlock-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 194, 112, 112]         111,744\n",
      "       BatchNorm2d-7        [-1, 194, 112, 112]             388\n",
      "         LeakyReLU-8        [-1, 194, 112, 112]               0\n",
      "          CNNBlock-9        [-1, 194, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 194, 56, 56]               0\n",
      "           Conv2d-11          [-1, 128, 56, 56]          24,832\n",
      "      BatchNorm2d-12          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-13          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-17          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-21          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-22          [-1, 128, 56, 56]               0\n",
      "           Conv2d-23          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-24          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-25          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-26          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-30          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-31          [-1, 128, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-34          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-35          [-1, 128, 28, 28]               0\n",
      "           Conv2d-36          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-37          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-38          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-42          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-43          [-1, 128, 28, 28]               0\n",
      "           Conv2d-44          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-45          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-46          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-47          [-1, 128, 28, 28]               0\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-50          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-54          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-58          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-59          [-1, 128, 28, 28]               0\n",
      "        MaxPool2d-60          [-1, 128, 14, 14]               0\n",
      "           Conv2d-61          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-62          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-63          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-64          [-1, 128, 14, 14]               0\n",
      "           Conv2d-65          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-66          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-67          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-68          [-1, 128, 14, 14]               0\n",
      "           Conv2d-69          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-70          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-71          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-72          [-1, 128, 14, 14]               0\n",
      "           Conv2d-73          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-74          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-75          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-76          [-1, 128, 14, 14]               0\n",
      "           Conv2d-77          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-78          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-79          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-80          [-1, 128, 14, 14]               0\n",
      "           Conv2d-81          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-82          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-83          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-84          [-1, 128, 14, 14]               0\n",
      "           Conv2d-85          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-86          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-87          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-88          [-1, 128, 14, 14]               0\n",
      "           Conv2d-89          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-90          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-91          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-92          [-1, 128, 14, 14]               0\n",
      "           Conv2d-93             [-1, 64, 7, 7]          73,728\n",
      "      BatchNorm2d-94             [-1, 64, 7, 7]             128\n",
      "        LeakyReLU-95             [-1, 64, 7, 7]               0\n",
      "         CNNBlock-96             [-1, 64, 7, 7]               0\n",
      "           Conv2d-97             [-1, 32, 7, 7]          18,432\n",
      "      BatchNorm2d-98             [-1, 32, 7, 7]              64\n",
      "        LeakyReLU-99             [-1, 32, 7, 7]               0\n",
      "        CNNBlock-100             [-1, 32, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 1,865,988\n",
      "Trainable params: 1,865,988\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 263.76\n",
      "Params size (MB): 7.12\n",
      "Estimated Total Size (MB): 273.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Information about architecture config:\n",
    "- Tuple is structured by (kernel_size, filters, stride, padding)\n",
    "- \"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "- List is structured by tuples and lastly int with number of repeats\n",
    "\"\"\"\n",
    "\n",
    "# original_yolo = [\n",
    "#     (7, 64, 2, 3),\n",
    "#     \"M\",\n",
    "#     (3, 192, 1, 1),\n",
    "#     \"M\",\n",
    "#     (1, 128, 1, 0),\n",
    "#     (3, 256, 1, 1),\n",
    "#     (1, 256, 1, 0),\n",
    "#     (3, 512, 1, 1),\n",
    "#     \"M\",\n",
    "#     [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "#     (1, 512, 1, 0),\n",
    "#     (3, 1024, 1, 1),\n",
    "#     \"M\",\n",
    "#     [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "#     (3, 1024, 1, 1),\n",
    "#     (3, 1024, 2, 1),\n",
    "#     (3, 1024, 1, 1),\n",
    "#     (3, 1024, 1, 1),\n",
    "# ]\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),  # 224\n",
    "    \"M\",  # 112\n",
    "    (3, 194, 1, 1),\n",
    "    \"M\",  # 56\n",
    "    [(1, 128, 1, 0), (3, 128, 1, 1), 2],\n",
    "    \"M\",  # 28\n",
    "    [(1, 128, 1, 0), (3, 128, 1, 1), 4],\n",
    "    \"M\", # 14\n",
    "    [(1, 128, 1, 0), (3, 128, 1, 1), 4],\n",
    "    (3, 64, 2, 1),  # 7\n",
    "    (3, 32, 1, 1),\n",
    "]\n",
    "\n",
    "summary(SimpleCNN(architecture_config, in_channels=3), (3, 448, 448))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 96, 221, 221]          14,208\n",
      "              ReLU-2         [-1, 96, 221, 221]               0\n",
      "         MaxPool2d-3         [-1, 96, 110, 110]               0\n",
      "            Conv2d-4         [-1, 16, 110, 110]           1,552\n",
      "              ReLU-5         [-1, 16, 110, 110]               0\n",
      "            Conv2d-6         [-1, 64, 110, 110]           1,088\n",
      "              ReLU-7         [-1, 64, 110, 110]               0\n",
      "            Conv2d-8         [-1, 64, 110, 110]           9,280\n",
      "              ReLU-9         [-1, 64, 110, 110]               0\n",
      "             Fire-10        [-1, 128, 110, 110]               0\n",
      "           Conv2d-11         [-1, 16, 110, 110]           2,064\n",
      "             ReLU-12         [-1, 16, 110, 110]               0\n",
      "           Conv2d-13         [-1, 64, 110, 110]           1,088\n",
      "             ReLU-14         [-1, 64, 110, 110]               0\n",
      "           Conv2d-15         [-1, 64, 110, 110]           9,280\n",
      "             ReLU-16         [-1, 64, 110, 110]               0\n",
      "             Fire-17        [-1, 128, 110, 110]               0\n",
      "           Conv2d-18         [-1, 32, 110, 110]           4,128\n",
      "             ReLU-19         [-1, 32, 110, 110]               0\n",
      "           Conv2d-20        [-1, 128, 110, 110]           4,224\n",
      "             ReLU-21        [-1, 128, 110, 110]               0\n",
      "           Conv2d-22        [-1, 128, 110, 110]          36,992\n",
      "             ReLU-23        [-1, 128, 110, 110]               0\n",
      "             Fire-24        [-1, 256, 110, 110]               0\n",
      "        MaxPool2d-25          [-1, 256, 55, 55]               0\n",
      "           Conv2d-26           [-1, 32, 55, 55]           8,224\n",
      "             ReLU-27           [-1, 32, 55, 55]               0\n",
      "           Conv2d-28          [-1, 128, 55, 55]           4,224\n",
      "             ReLU-29          [-1, 128, 55, 55]               0\n",
      "           Conv2d-30          [-1, 128, 55, 55]          36,992\n",
      "             ReLU-31          [-1, 128, 55, 55]               0\n",
      "             Fire-32          [-1, 256, 55, 55]               0\n",
      "           Conv2d-33           [-1, 48, 55, 55]          12,336\n",
      "             ReLU-34           [-1, 48, 55, 55]               0\n",
      "           Conv2d-35          [-1, 192, 55, 55]           9,408\n",
      "             ReLU-36          [-1, 192, 55, 55]               0\n",
      "           Conv2d-37          [-1, 192, 55, 55]          83,136\n",
      "             ReLU-38          [-1, 192, 55, 55]               0\n",
      "             Fire-39          [-1, 384, 55, 55]               0\n",
      "           Conv2d-40           [-1, 48, 55, 55]          18,480\n",
      "             ReLU-41           [-1, 48, 55, 55]               0\n",
      "           Conv2d-42          [-1, 192, 55, 55]           9,408\n",
      "             ReLU-43          [-1, 192, 55, 55]               0\n",
      "           Conv2d-44          [-1, 192, 55, 55]          83,136\n",
      "             ReLU-45          [-1, 192, 55, 55]               0\n",
      "             Fire-46          [-1, 384, 55, 55]               0\n",
      "           Conv2d-47           [-1, 64, 55, 55]          24,640\n",
      "             ReLU-48           [-1, 64, 55, 55]               0\n",
      "           Conv2d-49          [-1, 256, 55, 55]          16,640\n",
      "             ReLU-50          [-1, 256, 55, 55]               0\n",
      "           Conv2d-51          [-1, 256, 55, 55]         147,712\n",
      "             ReLU-52          [-1, 256, 55, 55]               0\n",
      "             Fire-53          [-1, 512, 55, 55]               0\n",
      "        MaxPool2d-54          [-1, 512, 27, 27]               0\n",
      "           Conv2d-55           [-1, 64, 27, 27]          32,832\n",
      "             ReLU-56           [-1, 64, 27, 27]               0\n",
      "           Conv2d-57          [-1, 256, 27, 27]          16,640\n",
      "             ReLU-58          [-1, 256, 27, 27]               0\n",
      "           Conv2d-59          [-1, 256, 27, 27]         147,712\n",
      "             ReLU-60          [-1, 256, 27, 27]               0\n",
      "             Fire-61          [-1, 512, 27, 27]               0\n",
      "================================================================\n",
      "Total params: 735,424\n",
      "Trainable params: 735,424\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 367.24\n",
      "Params size (MB): 2.81\n",
      "Estimated Total Size (MB): 372.34\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "SqueezeNet(\n  (features): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (3): Fire(\n      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (4): Fire(\n      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (5): Fire(\n      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (7): Fire(\n      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (8): Fire(\n      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (9): Fire(\n      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (10): Fire(\n      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (12): Fire(\n      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace=True)\n      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace=True)\n      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n    (2): ReLU(inplace=True)\n    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "# mobilenetv2 = torchvision.models.MobileNetV2(num_classes=20)\n",
    "# summary(mobilenetv2.features, (3, 448, 448))\n",
    "cnn = torchvision.models.squeezenet1_0()\n",
    "summary(cnn.features, (3, 448, 448))\n",
    "cnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels, split_size, num_boxes, num_classes):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.backbone = SimpleCNN(architecture_config, in_channels)\n",
    "\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * S * S, 128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, S * S * (C + B * 5)),\n",
    "        )\n",
    "        self.final_shape = (-1, (C + B * 5), S, S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        out = self.fcs(torch.flatten(x, start_dim=1))\n",
    "        out = out.view(self.final_shape)\n",
    "        return out\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Losses:\n",
    "\n",
    "    - Object exists: lambda_coord * sum((x - xhat)^2 + (y - yhat)^2)\n",
    "    - Object exists: lambda_coord * sum((sqrt(w) - sqrt(w_hat))^2 + (sqrt(h) - sqrt(h_hat))^2)\n",
    "    - Object exists: 1 * sum((confidence - confidence_hat)^2)\n",
    "    - No-object exists: lambda_no_object * sum((confidence - confidence_hat)^2)\n",
    "    - Object exists: sum((probability(c) - probability(c_hat))^2)\n",
    "\n",
    "    confidence = IoU\n",
    "    lambda_coord = 5\n",
    "    lambda_no_object = 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boxes: int,\n",
    "        num_classes: int,\n",
    "        lambda_coord: float,\n",
    "        lambda_object_exists: float,\n",
    "        lambda_no_object: float,\n",
    "        lambda_class: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Find the responsible cell-bbox pairs.\n",
    "\n",
    "        :param num_boxes: (B)\n",
    "        :param num_classes: (C)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_object_exists = lambda_object_exists\n",
    "        self.lambda_no_object = lambda_no_object\n",
    "        self.lambda_class = lambda_class\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    def forward(\n",
    "        self, preds: torch.Tensor, targets: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        - Responsible box is the one that has the highest IoU.\n",
    "\n",
    "        IoUs is a 0-1 tensor of shape (batch, B, S, S)\n",
    "        Responsibility is an index tensor of shape (batch, S, S)\n",
    "        object_exists is a 0,1 tensor of shape (batch, 1, S, S)\n",
    "\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return: a dict of all losses.\n",
    "        \"\"\"\n",
    "\n",
    "        ious = self._get_ious(preds.detach(), targets)  # shape: (batch, B, S, S)\n",
    "        responsibility = F.one_hot(\n",
    "            ious.argmax(dim=1), num_classes=self.num_boxes\n",
    "        )  # shape (batch, S, S, B)\n",
    "        object_exists = targets[:, self.num_classes]  # shape: (batch, S, S)\n",
    "        object_not_exists = 1 - object_exists\n",
    "\n",
    "        coords_loss = self._get_coords_loss(\n",
    "            preds, targets, object_exists, responsibility\n",
    "        )\n",
    "        confidence_loss = self._get_confidence_loss(\n",
    "            preds, ious, object_exists, responsibility\n",
    "        )\n",
    "        negative_confidence_loss = self._get_confidence_loss(\n",
    "            preds, ious, object_not_exists, responsibility\n",
    "        )\n",
    "        class_loss = self._get_class_loss(preds, targets, object_exists)\n",
    "\n",
    "        final_loss = (\n",
    "            coords_loss * self.lambda_coord\n",
    "            + confidence_loss * self.lambda_object_exists\n",
    "            + negative_confidence_loss * self.lambda_no_object\n",
    "            + class_loss * self.lambda_class\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": final_loss,\n",
    "            \"loss_coords\": coords_loss.detach(),\n",
    "            \"loss_confidence\": confidence_loss.detach(),\n",
    "            \"loss_confidence_negative\": negative_confidence_loss.detach(),\n",
    "            \"loss_class\": class_loss.detach(),\n",
    "        }\n",
    "\n",
    "    def _get_class_loss(self, preds, targets, object_exists):\n",
    "        c = preds[:, : self.num_classes]  # shape (batch, C, S, S)\n",
    "        c_hat = targets[:, : self.num_classes]  # shape (batch, C, S, S)\n",
    "\n",
    "        c_loss = self.mse(c_hat, c)  # shape (batch, C, S, S)\n",
    "        c_loss = c_loss.sum(dim=1)  # shape (batch, S, S)\n",
    "        c_loss = object_exists * c_loss\n",
    "        c_loss = c_loss.sum(dim=(1, 2)).mean(dim=0)\n",
    "        return c_loss\n",
    "\n",
    "    def _get_confidence_loss(self, preds, ious, object_exists, responsibility):\n",
    "        c_losses = []\n",
    "        for i in range(self.num_boxes):\n",
    "            c = ious[:, i]  # shape (batch, S, S)\n",
    "            c_hat = preds[:, self.num_classes + (i * 5)]  # shape (batch, S, S)\n",
    "\n",
    "            c_loss = self.mse(c_hat, c)\n",
    "            c_loss = object_exists * responsibility[..., i] * c_loss\n",
    "            c_loss = c_loss.sum(dim=(1, 2)).mean(dim=0)\n",
    "            c_losses.append(c_loss)\n",
    "        c_loss = torch.stack(c_losses).sum(dim=0)\n",
    "\n",
    "        return c_loss\n",
    "\n",
    "    def _get_coords_loss(self, preds, targets, object_exists, responsibility):\n",
    "        x = targets[:, self.num_classes + 1]  # shape (batch, S, S)\n",
    "        y = targets[:, self.num_classes + 2]\n",
    "        w = targets[:, self.num_classes + 3]\n",
    "        h = targets[:, self.num_classes + 4]\n",
    "        w_sqrt = torch.sqrt(torch.abs(w))\n",
    "        h_sqrt = torch.sqrt(torch.abs(h))\n",
    "\n",
    "        coords_losses = []  # shape (B,\n",
    "        for i in range(self.num_boxes):\n",
    "            start = self.num_classes + (i * 5)\n",
    "            x_hat = preds[:, start + 1]  # shape (batch, S, S)\n",
    "            y_hat = preds[:, start + 2]\n",
    "            w_hat = preds[:, start + 3]\n",
    "            h_hat = preds[:, start + 4]\n",
    "            w_hat_sqrt = torch.sqrt(torch.abs(w_hat))\n",
    "            h_hat_sqrt = torch.sqrt(torch.abs(h_hat))\n",
    "\n",
    "            xy_loss = self.mse(x_hat, x) + self.mse(y_hat, y)\n",
    "            wh_loss = self.mse(w_hat_sqrt, w_sqrt) + self.mse(h_hat_sqrt, h_sqrt)\n",
    "            coords_loss = object_exists * responsibility[..., i] * (xy_loss + wh_loss)\n",
    "            coords_loss = coords_loss.sum(dim=(1, 2)).mean(\n",
    "                dim=0\n",
    "            )  # average over batch, sum over rest.\n",
    "            coords_losses.append(coords_loss)\n",
    "        coords_loss = torch.stack(coords_losses).sum(dim=0)  # sum over B\n",
    "        return coords_loss\n",
    "\n",
    "    def _get_ious(self, preds, targets) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - When sum(target_[x,y,w,h]) is 0, iou is 0.\n",
    "        - w_cell, h_cell = 1/S\n",
    "        - w_image, h_image = 1\n",
    "\n",
    "        - Get x1, y1, x2, y2 for predicted and target boxes.\n",
    "            - x1 = midpoint_x - (width / 2)\n",
    "        - find box iou\n",
    "\n",
    "        :param preds: tensor of shape (batch, (C + B * 5), S, S)\n",
    "        :param targets: tensor of shape (batch, C+5, S, S)\n",
    "        :return: tensor of shape (batch, B, S, S)\n",
    "        \"\"\"\n",
    "\n",
    "        all_coords = []\n",
    "        for i in range(self.num_boxes):\n",
    "            start = self.num_classes + (i * 5) + 1\n",
    "            end = start + 4\n",
    "            coords = preds[:, start:end]\n",
    "            all_coords.append(coords)\n",
    "\n",
    "        coords = targets[:, self.num_classes + 1 :]  # shape: (batch, 1, S, S)\n",
    "        all_coords.append(coords)\n",
    "\n",
    "        all_coords = torch.stack(all_coords)  # shape (B+1, batch, 4, S, S)\n",
    "        all_coords = all_coords.moveaxis(2, 4)  # shape (B+1, batch, S, S, 4)\n",
    "\n",
    "        x = all_coords[..., 0:1]\n",
    "        y = all_coords[..., 1:2]\n",
    "        w = all_coords[..., 2:3]\n",
    "        h = all_coords[..., 3:4]\n",
    "\n",
    "        w_half = w / 2\n",
    "        h_half = h / 2\n",
    "\n",
    "        x1 = x - w_half\n",
    "        y1 = y - h_half\n",
    "        x2 = x + w_half\n",
    "        y2 = y + h_half\n",
    "\n",
    "        # x1 is of shape (B+1, batch, S, S, 1)\n",
    "        coords = torch.cat((x1, y1, x2, y2), dim=4)  # shape (B+1, batch, S, S, 4)\n",
    "\n",
    "        ious = []\n",
    "        for i in range(self.num_boxes):\n",
    "            iou = self.custom_ious(coords[i], coords[-1])  # shape (batch, S, S)\n",
    "            ious.append(iou)\n",
    "        ious = torch.stack(ious)  # shape (B, batch, S, S)\n",
    "        ious = ious.moveaxis(0, 1)  # shape (batch, B, S, S)\n",
    "\n",
    "        return ious\n",
    "\n",
    "    def custom_ious(self, boxes1, boxes2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs 1 to 1 iou\n",
    "        :param boxes1: tensor of shape (*N, 4)\n",
    "        :param boxes2: tensor of shape (*N, 4)\n",
    "        :return: tensor of shape *N\n",
    "        \"\"\"\n",
    "        assert boxes1.shape == boxes2.shape\n",
    "\n",
    "        ax1 = boxes1[..., 0]\n",
    "        ay1 = boxes1[..., 1]\n",
    "        ax2 = boxes1[..., 2]\n",
    "        ay2 = boxes1[..., 3]\n",
    "\n",
    "        bx1 = boxes2[..., 0]\n",
    "        by1 = boxes2[..., 1]\n",
    "        bx2 = boxes2[..., 2]\n",
    "        by2 = boxes2[..., 3]\n",
    "\n",
    "        x1 = self._max(ax1, bx1)\n",
    "        y1 = self._max(ay1, by1)\n",
    "        x2 = self._min(ax2, bx2)\n",
    "        y2 = self._min(ay2, by2)\n",
    "\n",
    "        zeros = torch.zeros_like(x1)\n",
    "        ones = torch.ones_like(x1)\n",
    "\n",
    "        side_x = self._max(zeros, x2 - x1)\n",
    "        side_y = self._max(zeros, y2 - y1)\n",
    "\n",
    "        intersection_area = side_x * side_y\n",
    "\n",
    "        box1_area = (ax2 - ax1) * (ay2 - ay1)\n",
    "        box2_area = (bx2 - bx1) * (by2 - by1)\n",
    "\n",
    "        epsilon = 1e-7\n",
    "        iou = intersection_area / (box1_area + box2_area - intersection_area + epsilon)\n",
    "        iou = self._min(ones, iou)  # shape (*N)\n",
    "        iou[bx2 - bx1 == 0] = 0.0  # Make IoU = 0 when width = 0\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def _max(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simply finds the max off the two tensors.\n",
    "        Shapes of the two tensors has to be same.\n",
    "        \"\"\"\n",
    "        return torch.amax(torch.stack([x, y]), dim=0)\n",
    "\n",
    "    def _min(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simply finds the max off the two tensors.\n",
    "        Shapes of the two tensors has to be same.\n",
    "        \"\"\"\n",
    "        return torch.amin(torch.stack([x, y]), dim=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\n",
    "class MyMetricCollection(MetricCollection):\n",
    "    def update_each(self, params: dict, **kwargs: Any) -> None:\n",
    "        \"\"\"params is a dict where key is the metric key and the values are tuples of positional arguments.\n",
    "        Keyword arguments (kwargs) will be filtered based on the signature of the individual metric.\n",
    "        \"\"\"\n",
    "        for key, m in self.items(keep_base=True):\n",
    "            if key in params:\n",
    "                args = params[key]\n",
    "                if type(args) is not tuple:\n",
    "                    args = (args,)\n",
    "                m_kwargs = m._filter_kwargs(**kwargs)\n",
    "                m.update(*args, **m_kwargs)\n",
    "\n",
    "    def compute(self):\n",
    "        result = super().compute()\n",
    "        self.reset()\n",
    "        return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "class YoloV1PL(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boxes: int,\n",
    "        num_classes: int,\n",
    "        in_channels: int,\n",
    "        grid_size: int,\n",
    "        lambda_coord: float,\n",
    "        lambda_object_exists: float,\n",
    "        lambda_no_object: float,\n",
    "        lambda_class: float,\n",
    "        **hp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        self.yolo_v1 = YoloV1(\n",
    "            in_channels=in_channels,\n",
    "            split_size=grid_size,\n",
    "            num_boxes=num_boxes,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        self.criterion = YoloV1Loss(\n",
    "            num_boxes=num_boxes,\n",
    "            num_classes=num_classes,\n",
    "            lambda_coord=lambda_coord,\n",
    "            lambda_object_exists=lambda_object_exists,\n",
    "            lambda_no_object=lambda_no_object,\n",
    "            lambda_class=lambda_class,\n",
    "        )\n",
    "\n",
    "        # --- metrics ---\n",
    "        self.metrics_train = MyMetricCollection(\n",
    "            {\n",
    "                \"loss\": AverageMeter(),\n",
    "                \"loss_coords\": AverageMeter(),\n",
    "                \"loss_confidence\": AverageMeter(),\n",
    "                \"loss_confidence_negative\": AverageMeter(),\n",
    "                \"loss_class\": AverageMeter(),\n",
    "            },\n",
    "            prefix=\"train/\",\n",
    "        )\n",
    "        self.metrics_val = self.metrics_train.clone(prefix=\"val/\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.yolo_v1(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hp[\"lr_initial\"])\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.hp[\"lr_decay_every\"],\n",
    "            gamma=self.hp[\"lr_decay_by\"],\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"learning_rate\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        losses = self.criterion(preds, targets)\n",
    "\n",
    "        # --- metrics and logging ----\n",
    "        print(losses)\n",
    "        self.metrics_train.update_each(losses)\n",
    "        self.log(\"train/loss_step\", losses[\"loss\"], prog_bar=True)\n",
    "        if batch_index == 0:\n",
    "            images_to_log = images[:self.hp['num_log_images']]\n",
    "            self.logger.experiment.log({'train/predictions': wandb.Image(images_to_log)})\n",
    "\n",
    "        return losses['loss']\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.log_dict(self.metrics_train.compute())\n",
    "\n",
    "    def validation_step(self, batch, _index):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        losses = self.criterion(preds, targets)\n",
    "        return losses\n",
    "\n",
    "    def validation_step_end(self, losses):\n",
    "        self.metrics_val.update_each(losses)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict(self.metrics_val.compute())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "         LeakyReLU-3         [-1, 64, 224, 224]               0\n",
      "          CNNBlock-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 194, 112, 112]         111,744\n",
      "       BatchNorm2d-7        [-1, 194, 112, 112]             388\n",
      "         LeakyReLU-8        [-1, 194, 112, 112]               0\n",
      "          CNNBlock-9        [-1, 194, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 194, 56, 56]               0\n",
      "           Conv2d-11          [-1, 128, 56, 56]          24,832\n",
      "      BatchNorm2d-12          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-13          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-17          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-21          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-22          [-1, 128, 56, 56]               0\n",
      "           Conv2d-23          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-24          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-25          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-26          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-30          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-31          [-1, 128, 28, 28]               0\n",
      "           Conv2d-32          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-34          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-35          [-1, 128, 28, 28]               0\n",
      "           Conv2d-36          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-37          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-38          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-42          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-43          [-1, 128, 28, 28]               0\n",
      "           Conv2d-44          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-45          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-46          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-47          [-1, 128, 28, 28]               0\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-50          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]          16,384\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-54          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-58          [-1, 128, 28, 28]               0\n",
      "         CNNBlock-59          [-1, 128, 28, 28]               0\n",
      "        MaxPool2d-60          [-1, 128, 14, 14]               0\n",
      "           Conv2d-61          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-62          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-63          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-64          [-1, 128, 14, 14]               0\n",
      "           Conv2d-65          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-66          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-67          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-68          [-1, 128, 14, 14]               0\n",
      "           Conv2d-69          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-70          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-71          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-72          [-1, 128, 14, 14]               0\n",
      "           Conv2d-73          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-74          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-75          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-76          [-1, 128, 14, 14]               0\n",
      "           Conv2d-77          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-78          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-79          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-80          [-1, 128, 14, 14]               0\n",
      "           Conv2d-81          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-82          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-83          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-84          [-1, 128, 14, 14]               0\n",
      "           Conv2d-85          [-1, 128, 14, 14]          16,384\n",
      "      BatchNorm2d-86          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-87          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-88          [-1, 128, 14, 14]               0\n",
      "           Conv2d-89          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-90          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-91          [-1, 128, 14, 14]               0\n",
      "         CNNBlock-92          [-1, 128, 14, 14]               0\n",
      "           Conv2d-93             [-1, 64, 7, 7]          73,728\n",
      "      BatchNorm2d-94             [-1, 64, 7, 7]             128\n",
      "        LeakyReLU-95             [-1, 64, 7, 7]               0\n",
      "         CNNBlock-96             [-1, 64, 7, 7]               0\n",
      "           Conv2d-97             [-1, 32, 7, 7]          18,432\n",
      "      BatchNorm2d-98             [-1, 32, 7, 7]              64\n",
      "        LeakyReLU-99             [-1, 32, 7, 7]               0\n",
      "        CNNBlock-100             [-1, 32, 7, 7]               0\n",
      "       SimpleCNN-101             [-1, 32, 7, 7]               0\n",
      "         Flatten-102                 [-1, 1568]               0\n",
      "          Linear-103                  [-1, 128]         200,832\n",
      "         Dropout-104                  [-1, 128]               0\n",
      "       LeakyReLU-105                  [-1, 128]               0\n",
      "          Linear-106                 [-1, 1470]         189,630\n",
      "          YoloV1-107             [-1, 30, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 2,256,450\n",
      "Trainable params: 2,256,450\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 263.81\n",
      "Params size (MB): 8.61\n",
      "Estimated Total Size (MB): 274.71\n",
      "----------------------------------------------------------------\n",
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | yolo_v1       | YoloV1             | 2.3 M \n",
      "1 | criterion     | YoloV1Loss         | 0     \n",
      "2 | metrics_train | MyMetricCollection | 0     \n",
      "3 | metrics_val   | MyMetricCollection | 0     \n",
      "-----------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.026     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "070c31d7e52c47c485fece5947156e1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: -1it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "108ae202eba647d7bcc096fbceed0d75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(50.6697, grad_fn=<AddBackward0>), 'loss_coords': tensor(9.1844), 'loss_confidence': tensor(0.0956), 'loss_confidence_negative': tensor(1.8735), 'loss_class': tensor(3.7152)}\n",
      "{'loss': tensor(83.5870, grad_fn=<AddBackward0>), 'loss_coords': tensor(15.6762), 'loss_confidence': tensor(0.0452), 'loss_confidence_negative': tensor(1.5584), 'loss_class': tensor(4.3815)}\n",
      "{'loss': tensor(104.3552, grad_fn=<AddBackward0>), 'loss_coords': tensor(19.7143), 'loss_confidence': tensor(0.0996), 'loss_confidence_negative': tensor(2.2591), 'loss_class': tensor(4.5548)}\n",
      "{'loss': tensor(40.1079, grad_fn=<AddBackward0>), 'loss_coords': tensor(7.3364), 'loss_confidence': tensor(0.1248), 'loss_confidence_negative': tensor(1.2225), 'loss_class': tensor(2.6900)}\n",
      "{'loss': tensor(100.5612, grad_fn=<AddBackward0>), 'loss_coords': tensor(19.2026), 'loss_confidence': tensor(0.0335), 'loss_confidence_negative': tensor(1.5894), 'loss_class': tensor(3.7200)}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51d55a6819a1489a84aef0298fd15ef7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(80.0152, grad_fn=<AddBackward0>), 'loss_coords': tensor(15.1356), 'loss_confidence': tensor(0.0398), 'loss_confidence_negative': tensor(1.9380), 'loss_class': tensor(3.3284)}\n",
      "{'loss': tensor(73.3488, grad_fn=<AddBackward0>), 'loss_coords': tensor(12.9016), 'loss_confidence': tensor(0.1014), 'loss_confidence_negative': tensor(1.2165), 'loss_class': tensor(8.1309)}\n",
      "{'loss': tensor(56.9850, grad_fn=<AddBackward0>), 'loss_coords': tensor(10.6492), 'loss_confidence': tensor(0.0288), 'loss_confidence_negative': tensor(1.4065), 'loss_class': tensor(3.0069)}\n",
      "{'loss': tensor(72.7366, grad_fn=<AddBackward0>), 'loss_coords': tensor(14.0065), 'loss_confidence': tensor(0.0153), 'loss_confidence_negative': tensor(1.3270), 'loss_class': tensor(2.0254)}\n",
      "{'loss': tensor(70.2779, grad_fn=<AddBackward0>), 'loss_coords': tensor(12.9014), 'loss_confidence': tensor(0.1235), 'loss_confidence_negative': tensor(1.5520), 'loss_class': tensor(4.8713)}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94235732bed84b69984fd2c6805afcec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3a3fa67af6a41c485339845a5066584"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b868a113af6a4791b6a6ae4bc8c72303"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n",
      "{'loss': tensor(nan, grad_fn=<AddBackward0>), 'loss_coords': tensor(nan), 'loss_confidence': tensor(nan), 'loss_confidence_negative': tensor(nan), 'loss_class': tensor(nan)}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a2dcb5c410342fea1549b4942661852"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hp = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr_initial\": 0.0001,\n",
    "    \"lr_decay_every\": 20,\n",
    "    \"lr_decay_by\": 0.99,\n",
    "    \"grid_size\": 7,\n",
    "    \"data_augment\": True,\n",
    "    \"num_boxes\": 2,\n",
    "    \"lambda_coord\": 5,\n",
    "    \"lambda_object_exists\": 1,\n",
    "    \"lambda_no_object\": 0.5,\n",
    "    \"lambda_class\": 1,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"output_path\": \"./output\",\n",
    "    \"val_split\": 0.1,\n",
    "    \"data_path\": \"./data\",\n",
    "    \"num_classes\": 20,\n",
    "    \"in_channels\": 3,\n",
    "    \"num_log_images\": 3,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"num_gpus\": 0\n",
    "}\n",
    "\n",
    "data_module = VocYoloDataModule(**config, **hp)\n",
    "model = YoloV1PL(**hp, **config).float()\n",
    "summary(model, (3, 448, 448))\n",
    "wandb_logger = WandbLogger(project=\"yolo_test\", log_model=False)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=config[\"num_gpus\"],\n",
    "    max_epochs=hp[\"epochs\"],\n",
    "    default_root_dir=config[\"output_path\"],\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "# wandb_logger.watch(model)\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(64.0, 64.0)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "h, w = 448, 448\n",
    "grid_rows, grid_cols = 7, 7\n",
    "cell_h = h / grid_rows\n",
    "cell_w = w / grid_cols\n",
    "\n",
    "cell_h, cell_w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n          [ 64.,  64.,  64.,  64.,  64.,  64.,  64.],\n          [128., 128., 128., 128., 128., 128., 128.],\n          [192., 192., 192., 192., 192., 192., 192.],\n          [256., 256., 256., 256., 256., 256., 256.],\n          [320., 320., 320., 320., 320., 320., 320.],\n          [384., 384., 384., 384., 384., 384., 384.]],\n \n         [[  0.,  64., 128., 192., 256., 320., 384.],\n          [  0.,  64., 128., 192., 256., 320., 384.],\n          [  0.,  64., 128., 192., 256., 320., 384.],\n          [  0.,  64., 128., 192., 256., 320., 384.],\n          [  0.,  64., 128., 192., 256., 320., 384.],\n          [  0.,  64., 128., 192., 256., 320., 384.],\n          [  0.,  64., 128., 192., 256., 320., 384.]]]),\n torch.Size([2, 7, 7]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = torch.arange(start=0, end=w, step=cell_w).view((1, grid_cols)).expand((grid_rows, grid_cols))\n",
    "rows = torch.arange(start=0, end=h, step=cell_h).view((grid_rows, 1)).expand((grid_rows, grid_cols))\n",
    "origins = torch.stack((rows, cols))\n",
    "origins, origins.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 64., 256.],\n        [ 64., 256.],\n        [ 64., 256.],\n        [ 64., 256.],\n        [ 64., 256.],\n        [ 64., 256.],\n        [ 64., 256.]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols[:, [1, 4]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "([21, 26], [22, 27], [23, 28], [24, 29])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_boxes = 2\n",
    "num_classes = 20\n",
    "\n",
    "idx = [], [], [], []\n",
    "idx_x, idx_y, idx_w, idx_h = idx\n",
    "\n",
    "for i in range(num_boxes):\n",
    "    for j in range(4):\n",
    "        idx[j].append(num_classes + (i * 5) + j + 1)\n",
    "\n",
    "idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "([21, 26], [22, 27], [23, 28], [24, 29])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 19 + 1 + 1\n",
    "y = x + 1\n",
    "w = y + 1\n",
    "h = w + 1\n",
    "x2 = h + 1 + 1\n",
    "y2 = x2 + 1\n",
    "w2 = y2 + 1\n",
    "h2 = w2 + 1\n",
    "\n",
    "([x, x2], [y, y2], [w, w2], [h, h2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}