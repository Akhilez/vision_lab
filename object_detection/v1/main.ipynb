{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a very quick implementation of object detection.\n",
    "\n",
    "\n",
    "1. Get the pascal voc dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets.voc import VOCDetection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "('1.9.0.post2', '0.10.0a0')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchvision.__version__\n",
    "# The working versions ('1.9.0.post2', '0.10.0a0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:2387: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOC_CLASSES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "\n",
    "HEIGHT = 448\n",
    "WIDTH = 448\n",
    "\n",
    "\n",
    "albument_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=HEIGHT, width=WIDTH, always_apply=True),\n",
    "        # A.RandomCrop(width=WIDTH, height=HEIGHT),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.2),\n",
    "        ToTensorV2(always_apply=True),\n",
    "        A.Lambda(image=lambda x, **kwargs: x / 255.0)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        min_visibility=0.5,\n",
    "        label_fields=['class_labels']\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def transform_targets_for_model(bboxes, classes):\n",
    "    \"\"\"\n",
    "    boxes: FloatTensor[N, 4] x1y1x2y2\n",
    "    labels: Int64Tensor[N]\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'boxes': torch.FloatTensor(bboxes),\n",
    "        'labels': torch.LongTensor(classes),\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_targets_for_augmentation(targets: dict) -> Tuple[List[Tuple[int, int, int, int]], List[int]]:\n",
    "    classes = []\n",
    "    boxes = []\n",
    "    for object in targets['annotation']['object']:\n",
    "        class_index = VOC_CLASSES.index(object['name'])\n",
    "        classes.append(class_index)\n",
    "\n",
    "        box = object['bndbox']\n",
    "        box = tuple(int(box[key]) for key in ['xmin', 'ymin', 'xmax', 'ymax'])\n",
    "        boxes.append(box)\n",
    "\n",
    "    return boxes, classes\n",
    "\n",
    "\n",
    "def transforms_fn(image, targets):\n",
    "    boxes, classes = transform_targets_for_augmentation(targets)\n",
    "\n",
    "    transformed = albument_transforms(\n",
    "        image=np.array(image),\n",
    "        bboxes=boxes,\n",
    "        class_labels=classes,\n",
    "    )\n",
    "\n",
    "    transformed_image = transformed['image']\n",
    "    transformed_bboxes = transformed['bboxes']\n",
    "    transformed_class_labels = transformed['class_labels']\n",
    "\n",
    "    transformed_targets = transform_targets_for_model(\n",
    "        transformed_bboxes, transformed_class_labels\n",
    "    )\n",
    "    return transformed_image, transformed_targets\n",
    "\n",
    "\n",
    "def collate_fn_voc(batch: List[Tuple[torch.Tensor, dict]]) -> Tuple[list, Tuple[list, list]]:\n",
    "    \"\"\"\n",
    "    :param batch: list of tuple of image and and dict targets\n",
    "    :return: images are batched into a tensor, rest are lists\n",
    "    \"\"\"\n",
    "    # batch = [transforms_fn(image, target) for image, target in batch]\n",
    "    images = []\n",
    "    targets = []\n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    return images, targets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "target is of the following shape:\n",
    "\n",
    "```yaml\n",
    "\n",
    "annotation:\n",
    "  filename: 2009_004972.jpg\n",
    "  folder: VOC2012\n",
    "  object:\n",
    "    - name: bicycle\n",
    "      bndbox:\n",
    "        xmax: 471\n",
    "        xmin: 54\n",
    "        ymax: 336\n",
    "        ymin: 39\n",
    "      difficult: 0\n",
    "      occluded: 0\n",
    "      pose: Left\n",
    "      trucated: 0\n",
    "  segmented: 0\n",
    "  size:\n",
    "    depth: 3\n",
    "    height: 375\n",
    "    width: 500\n",
    "  source:\n",
    "    annotation: PASCAL VOC2009\n",
    "    database: The VOC2009 Database\n",
    "    image: flickr\n",
    "```\n",
    "\n",
    "But it needs to be in this shape:\n",
    "\n",
    "```yaml\n",
    "boxese: FloatTensor[N, 4] x1y1x2y2\n",
    "labels: Int64Tensor[N]\n",
    "image_id:\n",
    "area:\n",
    "iscrowd:\n",
    "masks:\n",
    "keypoints:\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "class DetectionMetrics:\n",
    "    \"\"\"\n",
    "    This class keeps track of all the metrics during training and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics_to_track = [\n",
    "            #'accuracy',\n",
    "            #'iou',\n",
    "        ]\n",
    "        self.epoch_metrics = {key: 0.0 for key in self.metrics_to_track}\n",
    "        self.n_batches = 0\n",
    "\n",
    "    def step_batch(\n",
    "        self,\n",
    "        **other,\n",
    "    ):\n",
    "        batch_metrics = other\n",
    "\n",
    "        self._log_batch(batch_metrics)\n",
    "        return batch_metrics\n",
    "\n",
    "    def step_epoch(self) -> dict:\n",
    "        for key in self.epoch_metrics:\n",
    "            self.epoch_metrics[key] /= self.n_batches\n",
    "            metrics = deepcopy(self.epoch_metrics)\n",
    "            self.clear()\n",
    "            return metrics\n",
    "\n",
    "    def _log_batch(self, batch_metrics: dict):\n",
    "        for key in batch_metrics:\n",
    "            if key not in self.epoch_metrics:\n",
    "                self.epoch_metrics[key] = 0\n",
    "            self.epoch_metrics[key] += float(batch_metrics[key])\n",
    "        self.n_batches += 1\n",
    "\n",
    "    def clear(self):\n",
    "        self.epoch_metrics = {key: 0.0 for key in self.metrics_to_track}\n",
    "        self.n_batches = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_model(num_classes: int):\n",
    "\n",
    "    backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "    backbone.out_channels = 1280\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((64, 128, 256),),\n",
    "        aspect_ratios=((1.0,),)\n",
    "    )\n",
    "\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=2,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "\n",
    "    num_classes = num_classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "\n",
    "def _save_model(model, cfg: dict, postfix=None):\n",
    "    postfix = f\"__{postfix}\" if postfix is not None else \"\"\n",
    "    file_name = f\"model{postfix}.pth\"\n",
    "    checkpoint_path = join(cfg['output_path'], \"checkpoints\", file_name)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "\n",
    "def evaluate_dataset(model, data_loader, metrics, optimizer):\n",
    "    # TODO: Change this to eval()\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        # TODO: I don't know how to do this in eval mode.\n",
    "        losses = model(images)\n",
    "        loss = torch.sum(losses.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        metrics.step_batch(\n",
    "            loss=float(loss),\n",
    "        )\n",
    "    scores = metrics.step_epoch()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def log_metrics(logger, scores: dict, prefix: str, step):\n",
    "    for key in scores:\n",
    "        logger.add_scalar(f'{prefix}/{key}', scores[key], step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'data_path': './data',\n",
    "    'valset_size': 0.1,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate.initial': 0.0001,\n",
    "    'learning_rate.decay_every': 30,\n",
    "    'learning_rate.decay_by': 0.3,\n",
    "    'output_path': './output',\n",
    "    'model_save_frequency': 5,\n",
    "}\n",
    "\n",
    "makedirs(join(cfg['output_path'], \"checkpoints\"), exist_ok=True)\n",
    "logger = SummaryWriter()\n",
    "metrics = DetectionMetrics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "    VOCDetection(\n",
    "        root='../data',\n",
    "        year='2012',\n",
    "        image_set='train',\n",
    "        download=False,\n",
    "        transforms=transforms_fn,\n",
    "    ),\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_voc,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    VOCDetection(\n",
    "        root='../data',\n",
    "        year='2012',\n",
    "        image_set='val',\n",
    "        download=False,\n",
    "        transforms=transforms_fn,\n",
    "    ),\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_voc,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = get_model(len(VOC_CLASSES))\n",
    "\n",
    "if cfg.get('checkpoint'):\n",
    "    model.load(cfg['checkpoint'])\n",
    "\n",
    "model = model.float().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=cfg['learning_rate.initial']\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=cfg['learning_rate.decay_every'],\n",
    "    gamma=cfg['learning_rate.decay_by'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 5/2859 [01:53<17:55:12, 22.60s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "In training mode, targets should be passed",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/sx/r365fxd54690mcwndcx74zth0000gn/T/ipykernel_52403/1971571658.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0m_save_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m \u001B[0mtest_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluate_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_hparams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcfg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_scores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/sx/r365fxd54690mcwndcx74zth0000gn/T/ipykernel_52403/3177749050.py\u001B[0m in \u001B[0;36mevaluate_dataset\u001B[0;34m(model, data_loader, metrics, optimizer)\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;31m# with torch.no_grad():\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0;31m# TODO: I don't know how to do this in eval mode.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1051\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1052\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, images, targets)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \"\"\"\n\u001B[1;32m     56\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtargets\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"In training mode, targets should be passed\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mtargets\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: In training mode, targets should be passed"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        with tqdm(train_loader, unit=\"batch\") as batches_bar:\n",
    "            batches_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            i = 0\n",
    "            for images, targets in batches_bar:\n",
    "                i += 1\n",
    "                if i > 10:\n",
    "                    break\n",
    "\n",
    "                losses = model(images, targets)\n",
    "                loss = sum(losses.values())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # -------- Metrics ------------\n",
    "\n",
    "                batch_metrics = metrics.step_batch(\n",
    "                    loss=float(loss),\n",
    "                    learning_rate=float(lr_scheduler.get_last_lr()[0]),\n",
    "                    **losses\n",
    "                )\n",
    "                batches_bar.set_postfix(\n",
    "                    loss=batch_metrics['loss'],\n",
    "                )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        if (epoch + 1) % cfg['model_save_frequency'] == 0:\n",
    "            _save_model(model, cfg, epoch)\n",
    "\n",
    "        scores = metrics.step_epoch()\n",
    "        log_metrics(logger, scores, 'train', epoch)\n",
    "\n",
    "        test_scores = evaluate_dataset(model, test_loader, metrics, optimizer)\n",
    "        log_metrics(logger, test_scores, 'val', epoch)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping early.\")\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "_save_model(model, cfg)\n",
    "test_scores = evaluate_dataset(model, test_loader, metrics, optimizer)\n",
    "logger.add_hparams(cfg, test_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}