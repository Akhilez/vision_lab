{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is image classification with few good practices.\n",
    "\n",
    "Uses the following frameworks:\n",
    "\n",
    "- PyTorch Lightning\n",
    "- TorchMetrics\n",
    "-"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MnistDataset(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        data_path: str,\n",
    "        val_split: float,\n",
    "        dataloader_num_workers: int = 0,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = 28\n",
    "        self.w = 28\n",
    "        self.dims = (1, self.h, self.w)\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.num_workers = dataloader_num_workers\n",
    "\n",
    "        self.transform_train_val, self.transform_test = self._get_transforms()\n",
    "\n",
    "        self.mnist_train, self.mnist_val, self.mnist_test = None, None, None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_path, train=True, download=True)\n",
    "        MNIST(self.data_path, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        :param stage: One of {fit, validate, test}. None = all 3\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(\n",
    "                self.data_path, train=True, transform=self.transform_train_val\n",
    "            )\n",
    "            full_size = len(mnist_full)\n",
    "            val_size = int(full_size * self.val_split)\n",
    "            self.mnist_train, self.mnist_val = random_split(\n",
    "                mnist_full,\n",
    "                [full_size - val_size, val_size],\n",
    "                generator=torch.Generator().manual_seed(42),\n",
    "            )\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_path, train=False, transform=self.transform_test\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def _get_transforms(self):\n",
    "        resize_transform = A.Compose(\n",
    "            [\n",
    "                A.PadIfNeeded(\n",
    "                    min_height=self.h, min_width=self.w, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.RandomCrop(self.h, self.w),\n",
    "            ]\n",
    "        )\n",
    "        augmentations = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        def _normalizer(image, **_):\n",
    "            return image / 255.0\n",
    "\n",
    "        compatible = A.Compose(\n",
    "            [\n",
    "                ToTensorV2(always_apply=True),\n",
    "                A.Lambda(image=_normalizer),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transforms_train_val_ = A.Compose([resize_transform, augmentations, compatible])\n",
    "        transforms_test_ = A.Compose([resize_transform, compatible])\n",
    "\n",
    "        def transforms_train_val(image):\n",
    "            return transforms_train_val_(image=np.array(image))[\"image\"]\n",
    "\n",
    "        def transforms_test(image):\n",
    "            return transforms_test_(image=np.array(image))[\"image\"]\n",
    "\n",
    "        return transforms_train_val, transforms_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        architecture: List[Union[tuple, str, list]],\n",
    "        in_channels: int,\n",
    "    ):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        for module in architecture:\n",
    "            if type(module) is tuple:\n",
    "                layers.append(self._get_cnn_block(module, in_channels))\n",
    "                in_channels = module[1]\n",
    "            elif module == \"M\":\n",
    "                layers.append(\n",
    "                    nn.MaxPool2d(\n",
    "                        kernel_size=(2, 2),\n",
    "                        stride=(2, 2),\n",
    "                    )\n",
    "                )\n",
    "            elif type(module) is list:\n",
    "                for i in range(module[-1]):\n",
    "                    for j in range(len(module) - 1):\n",
    "                        layers.append(self._get_cnn_block(module[j], in_channels))\n",
    "                        in_channels = module[j][1]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_cnn_block(module: tuple, in_channels):\n",
    "        kernel_size, filters, stride, padding = module\n",
    "        return CNNBlock(\n",
    "            in_channels,\n",
    "            filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MnistModel(pl.LightningModule):\n",
    "    def __init__(self, **hp):\n",
    "        super().__init__()\n",
    "        architecture = [\n",
    "            # (kernel_size, filters, stride, padding)\n",
    "            (3, 10, 1, 1),\n",
    "            \"M\",\n",
    "            (3, 20, 1, 1),\n",
    "            \"M\",\n",
    "            (3, 40, 1, 1),\n",
    "            # \"M\",\n",
    "            (3, 80, 1, 1),\n",
    "            (3, 10, 1, 0),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            SimpleCNN(architecture, in_channels=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.hp = hp\n",
    "        self.save_hyperparameters(hp)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Metrics\n",
    "        self.accuracy_train = pl.metrics.Accuracy()\n",
    "        self.accuracy_val = pl.metrics.Accuracy()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        preds = self(images)\n",
    "        loss = self.criterion(preds, targets)\n",
    "        return {'loss': loss, 'preds': preds, 'inputs': images, 'targets': targets}\n",
    "\n",
    "    def training_step_end(self, outs: dict):\n",
    "        self.accuracy_train(outs[\"preds\"], outs[\"targets\"])\n",
    "        self.log({\"train/acc_step\": self.accuracy_train})\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._evaluate(batch, batch_idx, \"val\")\n",
    "\n",
    "    def _evaluate(self, batch, batch_idx, prefix: str):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(f\"{prefix}/loss\", loss)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28]) torch.Size([2])\n",
      "torch.Size([2, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-jhk0fkj8/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /tmp/pip-req-build-jhk0fkj8/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def debug1():\n",
    "    config = {\n",
    "        \"data_path\": \"../data\",\n",
    "        \"val_split\": 0.3,\n",
    "        \"batch_size\": 2,\n",
    "        \"output_path\": \"./output\",\n",
    "        \"model_save_frequency\": 5,\n",
    "        \"dataloader_num_workers\": 0,\n",
    "    }\n",
    "    dataset = MnistDataset(**config)\n",
    "\n",
    "    dataset.prepare_data()\n",
    "    dataset.setup()\n",
    "    loader = dataset.train_dataloader()\n",
    "    for image, classes in loader:\n",
    "        print(image.shape, classes.shape)\n",
    "        break\n",
    "\n",
    "    hp = {\n",
    "        \"epochs\": 10,\n",
    "        \"lr_initial\": 0.0001,\n",
    "        \"lr_decay_every\": 30,\n",
    "        \"lr_decay_by\": 0.3,\n",
    "    }\n",
    "    model = MnistModel(**hp, **config)\n",
    "    y = model(image)\n",
    "    print(y.shape)\n",
    "\n",
    "debug1()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33makhilez\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.12.2<br/>\n                Syncing run <strong style=\"color:#cdcd00\">bright-wood-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akhilez/classification_test\" target=\"_blank\">https://wandb.ai/akhilez/classification_test</a><br/>\n                Run page: <a href=\"https://wandb.ai/akhilez/classification_test/runs/305fzn4y\" target=\"_blank\">https://wandb.ai/akhilez/classification_test/runs/305fzn4y</a><br/>\n                Run data is saved locally in <code>/Users/akhil/code/vision_lab/classification/wandb/run-20210921_184010-305fzn4y</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 45.4 K\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "45.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "45.4 K    Total params\n",
      "0.182     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2370d9e9e594b729b2d0baacf8a6528"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/dl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: -1it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bb74ab1d93042f39e1ef3b3492bb6e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4900d20278714552913ba2d6a964a387"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dd6fa11e1764f57845cb0b968e63172"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def train():\n",
    "\n",
    "    hp = {\n",
    "        \"epochs\": 10,\n",
    "        \"lr_initial\": 0.0001,\n",
    "        \"lr_decay_every\": 30,\n",
    "        \"lr_decay_by\": 0.3,\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        \"data_path\": \"../data\",\n",
    "        \"val_split\": 0.3,\n",
    "        \"batch_size\": 64,\n",
    "        \"output_path\": \"./output\",\n",
    "        \"model_save_frequency\": 5,\n",
    "        \"dataloader_num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    dataset = MnistDataset(**config)\n",
    "    model = MnistModel(**hp, **config)\n",
    "    wandb_logger = WandbLogger(project='classification_test', log_model=True)\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=0,\n",
    "        max_epochs=hp[\"epochs\"],\n",
    "        default_root_dir=config[\"output_path\"],\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "    # wandb_logger.watch(model)\n",
    "\n",
    "    trainer.fit(model, datamodule=dataset)\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.7500)\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = pl.metrics.Accuracy()\n",
    "accuracy_train(  # 0.5\n",
    "    torch.tensor([[0.0, 0.1, 0.9], [0.0, 0.8, 0.2]]),\n",
    "    torch.tensor([2, 2])\n",
    ")\n",
    "print(accuracy_train.compute())\n",
    "accuracy_train(  # 1\n",
    "    torch.tensor([[0.0, 0.1, 0.9], [0.0, 0.8, 0.2]]),\n",
    "    torch.tensor([2, 1])\n",
    ")\n",
    "print(accuracy_train.compute())\n",
    "accuracy_train(  # 3/4\n",
    "    torch.tensor([[0.0, 0.1, 0.9], [0.0, 0.8, 0.2], [0.0, 0.1, 0.9], [0.0, 0.8, 0.2]]),\n",
    "    torch.tensor([2, 0, 0, 0])\n",
    ")\n",
    "print(accuracy_train.compute())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}